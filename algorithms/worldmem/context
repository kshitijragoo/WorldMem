hemkritragoonath@Hans-Air ~/D/C/WorldMem (main)> tree                                                                                                           (base) 

.

├── algorithms

│   ├── __init__.py

│   ├── common

│   │   ├── __init__.py

│   │   ├── base_algo.py

│   │   ├── base_pytorch_algo.py

│   │   ├── metrics

│   │   │   ├── __init__.py

│   │   │   ├── fid.py

│   │   │   ├── fvd.py

│   │   │   └── lpips.py

│   │   ├── models

│   │   │   ├── __init__.py

│   │   │   ├── cnn.py

│   │   │   └── mlp.py

│   │   └── README.md

│   ├── README.md

│   └── worldmem

│       ├── __init__.py

│       ├── df_base.py

│       ├── df_video.py

│       ├── models

│       │   ├── attention.py

│       │   ├── cameractrl_module.py

│       │   ├── diffusion.py

│       │   ├── dit.py

│       │   ├── pose_prediction.py

│       │   ├── dinov3_feature_extractor.py

│       │   ├── rotary_embedding_torch.py

│       │   ├── utils.py

│       │   └── vae.py

│       └── pose_prediction.py

├── app.py

├── assets

│   ├── desert.png

│   ├── ice_plains.png

│   ├── place.png

│   ├── plains.png

│   ├── rain_sunflower_plains.png

│   ├── savanna.png

│   ├── sunflower_plains.png

│   └── worldmem_logo.png

├── configurations

│   ├── algorithm

│   │   ├── base_algo.yaml

│   │   ├── base_pytorch_algo.yaml

│   │   ├── df_base.yaml

│   │   └── df_video_worldmemminecraft.yaml

│   ├── dataset

│   │   ├── base_dataset.yaml

│   │   ├── base_video.yaml

│   │   └── video_minecraft.yaml

│   ├── experiment

│   │   ├── base_experiment.yaml

│   │   ├── base_pytorch.yaml

│   │   └── exp_video.yaml

│   ├── huggingface.yaml

│   └── training.yaml

├── datasets

│   ├── __init__.py

│   ├── README.md

│   └── video

│       ├── __init__.py

│       ├── base_video_dataset.py

│       └── minecraft_video_dataset.py

├── experiments

│   ├── __init__.py

│   ├── exp_base.py

│   ├── exp_video.py

│   └── README.md

├── infer.sh

├── LICENSE.md

├── main.py

├── README.md

├── requirements.txt

├── train_stage_1.sh

├── train_stage_2.sh

├── train_stage_3.sh

└── utils

    ├── __init__.py

    ├── ckpt_utils.py

    ├── cluster_utils.py

    ├── distributed_utils.py

    ├── logging_utils.py

    ├── print_utils.py

    ├── README.md

    └── wandb_utils.py



16 directories, 71 files











——————————————————————————————————————————————————

#df_base.py

"""

This repo is forked from [Boyuan Chen](https://boyuan.space/)'s research 

template [repo](https://github.com/buoyancy99/research-template). 

By its MIT license, you must keep the above sentence in `README.md` 

and the `LICENSE` file to credit the author.

"""



from typing import Optional

from tqdm import tqdm

from omegaconf import DictConfig

import numpy as np

import torch

import torch.nn.functional as F

from typing import Any

from einops import rearrange



from lightning.pytorch.utilities.types import STEP_OUTPUT



from algorithms.common.base_pytorch_algo import BasePytorchAlgo

from .models.diffusion import Diffusion





class DiffusionForcingBase(BasePytorchAlgo):

    def __init__(self, cfg: DictConfig):

        self.cfg = cfg

        self.x_shape = cfg.x_shape

        self.frame_stack = cfg.frame_stack

        self.x_stacked_shape = list(self.x_shape)

        self.x_stacked_shape[0] *= cfg.frame_stack

        self.guidance_scale = cfg.guidance_scale

        self.context_frames = cfg.context_frames

        self.chunk_size = cfg.chunk_size

        self.action_cond_dim = cfg.action_cond_dim

        self.causal = cfg.causal



        self.uncertainty_scale = cfg.uncertainty_scale

        self.timesteps = cfg.diffusion.timesteps

        self.sampling_timesteps = cfg.diffusion.sampling_timesteps

        self.clip_noise = cfg.diffusion.clip_noise



        self.cfg.diffusion.cum_snr_decay = self.cfg.diffusion.cum_snr_decay ** (self.frame_stack * cfg.frame_skip)



        self.validation_step_outputs = []

        super().__init__(cfg)



    def _build_model(self):

        self.diffusion_model = Diffusion(

            x_shape=self.x_stacked_shape,

            action_cond_dim=self.action_cond_dim,

            is_causal=self.causal,

            cfg=self.cfg.diffusion,

        )

        self.register_data_mean_std(self.cfg.data_mean, self.cfg.data_std)



    def configure_optimizers(self):

        params = tuple(self.diffusion_model.parameters())

        optimizer_dynamics = torch.optim.AdamW(

            params, lr=self.cfg.lr, weight_decay=self.cfg.weight_decay, betas=self.cfg.optimizer_beta

        )

        return optimizer_dynamics



    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):

        # update params

        optimizer.step(closure=optimizer_closure)



        # manually warm up lr without a scheduler

        if self.trainer.global_step < self.cfg.warmup_steps:

            lr_scale = min(1.0, float(self.trainer.global_step + 1) / self.cfg.warmup_steps)

            for pg in optimizer.param_groups:

                pg["lr"] = lr_scale * self.cfg.lr



    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:

        xs, conditions, masks = self._preprocess_batch(batch)



        rand_length = torch.randint(3,xs.shape[0]-2, (1,))[0].item()

        xs = torch.cat([xs[:rand_length], xs[rand_length-3:rand_length-1]])

        conditions = torch.cat([conditions[:rand_length], conditions[rand_length-3:rand_length-1]])

        masks = torch.cat([masks[:rand_length], masks[rand_length-3:rand_length-1]])

        noise_levels=self._generate_noise_levels(xs)

        noise_levels[:rand_length] = 15 # stable_noise_levels

        noise_levels[rand_length+1:] = 15 # stable_noise_levels



        xs_pred, loss = self.diffusion_model(xs, conditions, noise_levels=noise_levels)

        loss = self.reweight_loss(loss, masks)



        # log the loss

        if batch_idx % 20 == 0:

            self.log("training/loss", loss)



        xs = self._unstack_and_unnormalize(xs)

        xs_pred = self._unstack_and_unnormalize(xs_pred)



        output_dict = {

            "loss": loss,

            "xs_pred": xs_pred,

            "xs": xs,

        }



        return output_dict



    @torch.no_grad()

    def validation_step(self, batch, batch_idx, namespace="validation") -> STEP_OUTPUT:

        xs, conditions, masks = self._preprocess_batch(batch)

        n_frames, batch_size, *_ = xs.shape

        xs_pred = []

        curr_frame = 0



        # context

        n_context_frames = self.context_frames // self.frame_stack

        xs_pred = xs[:n_context_frames].clone()

        curr_frame += n_context_frames



        if self.condtion_similar_length:

            n_frames -= self.condtion_similar_length



        pbar = tqdm(total=n_frames, initial=curr_frame, desc="Sampling")

        while curr_frame < n_frames:

            if self.chunk_size > 0:

                horizon = min(n_frames - curr_frame, self.chunk_size)

            else:

                horizon = n_frames - curr_frame

            assert horizon <= self.n_tokens, "horizon exceeds the number of tokens."

            scheduling_matrix = self._generate_scheduling_matrix(horizon)



            chunk = torch.randn((horizon, batch_size, *self.x_stacked_shape), device=self.device)

            chunk = torch.clamp(chunk, -self.clip_noise, self.clip_noise)

            xs_pred = torch.cat([xs_pred, chunk], 0)



            # sliding window: only input the last n_tokens frames

            start_frame = max(0, curr_frame + horizon - self.n_tokens)



            pbar.set_postfix(

                {

                    "start": start_frame,

                    "end": curr_frame + horizon,

                }

            )



            if self.condtion_similar_length:

                xs_pred = torch.cat([xs_pred, xs[curr_frame-self.condtion_similar_length:curr_frame].clone()], 0)



            for m in range(scheduling_matrix.shape[0] - 1):



                from_noise_levels = np.concatenate((np.zeros((curr_frame,), dtype=np.int64), scheduling_matrix[m]))[

                    :, None

                ].repeat(batch_size, axis=1)

                to_noise_levels = np.concatenate(

                    (

                        np.zeros((curr_frame,), dtype=np.int64),

                        scheduling_matrix[m + 1],

                    )

                )[

                    :, None

                ].repeat(batch_size, axis=1)



                if self.condtion_similar_length:

                    from_noise_levels = np.concatenate([from_noise_levels, np.array([[0,0,0,0]*self.condtion_similar_length])], axis=0)

                    to_noise_levels = np.concatenate([to_noise_levels, np.array([[0,0,0,0]*self.condtion_similar_length])], axis=0)



                from_noise_levels = torch.from_numpy(from_noise_levels).to(self.device)

                to_noise_levels = torch.from_numpy(to_noise_levels).to(self.device)



                # update xs_pred by DDIM or DDPM sampling

                # input frames within the sliding window



                try:

                    input_condition = conditions[start_frame : curr_frame + horizon].clone()

                except:

                    import pdb;pdb.set_trace()

                if self.condtion_similar_length:

                    input_condition = torch.cat([conditions[start_frame : curr_frame + horizon], conditions[-self.condtion_similar_length:]], dim=0)

                xs_pred[start_frame:] = self.diffusion_model.sample_step(

                    xs_pred[start_frame:],

                    input_condition,

                    from_noise_levels[start_frame:],

                    to_noise_levels[start_frame:],

                )



            if self.condtion_similar_length:

                xs_pred = xs_pred[:-self.condtion_similar_length]



            curr_frame += horizon

            pbar.update(horizon)



        if self.condtion_similar_length:

            xs = xs[:-self.condtion_similar_length]

        # FIXME: loss

        loss = F.mse_loss(xs_pred, xs, reduction="none")

        loss = self.reweight_loss(loss, masks)

        self.validation_step_outputs.append((xs_pred.detach().cpu(), xs.detach().cpu()))



        return loss



    def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:

        return self.validation_step(*args, **kwargs, namespace="test")



    def test_epoch_end(self) -> None:

        self.on_validation_epoch_end(namespace="test")



    def _generate_noise_levels(self, xs: torch.Tensor, masks: Optional[torch.Tensor] = None) -> torch.Tensor:

        """

        Generate noise levels for training.

        """

        num_frames, batch_size, *_ = xs.shape

        match self.cfg.noise_level:

            case "random_all":  # entirely random noise levels

                noise_levels = torch.randint(0, self.timesteps, (num_frames, batch_size), device=xs.device)

            case "same":

                noise_levels = torch.randint(0, self.timesteps, (num_frames, batch_size), device=xs.device)

                noise_levels[1:] = noise_levels[0]



        if masks is not None:

            # for frames that are not available, treat as full noise

            discard = torch.all(~rearrange(masks.bool(), "(t fs) b -> t b fs", fs=self.frame_stack), -1)

            noise_levels = torch.where(discard, torch.full_like(noise_levels, self.timesteps - 1), noise_levels)



        return noise_levels



    def _generate_scheduling_matrix(self, horizon: int):

        match self.cfg.scheduling_matrix:

            case "pyramid":

                return self._generate_pyramid_scheduling_matrix(horizon, self.uncertainty_scale)

            case "full_sequence":

                return np.arange(self.sampling_timesteps, -1, -1)[:, None].repeat(horizon, axis=1)

            case "autoregressive":

                return self._generate_pyramid_scheduling_matrix(horizon, self.sampling_timesteps)

            case "trapezoid":

                return self._generate_trapezoid_scheduling_matrix(horizon, self.uncertainty_scale)



    def _generate_pyramid_scheduling_matrix(self, horizon: int, uncertainty_scale: float):

        height = self.sampling_timesteps + int((horizon - 1) * uncertainty_scale) + 1

        scheduling_matrix = np.zeros((height, horizon), dtype=np.int64)

        for m in range(height):

            for t in range(horizon):

                scheduling_matrix[m, t] = self.sampling_timesteps + int(t * uncertainty_scale) - m



        return np.clip(scheduling_matrix, 0, self.sampling_timesteps)



    def _generate_trapezoid_scheduling_matrix(self, horizon: int, uncertainty_scale: float):

        height = self.sampling_timesteps + int((horizon + 1) // 2 * uncertainty_scale)

        scheduling_matrix = np.zeros((height, horizon), dtype=np.int64)

        for m in range(height):

            for t in range((horizon + 1) // 2):

                scheduling_matrix[m, t] = self.sampling_timesteps + int(t * uncertainty_scale) - m

                scheduling_matrix[m, -t] = self.sampling_timesteps + int(t * uncertainty_scale) - m



        return np.clip(scheduling_matrix, 0, self.sampling_timesteps)



    def reweight_loss(self, loss, weight=None):

        # Note there is another part of loss reweighting (fused_snr) inside the Diffusion class!

        loss = rearrange(loss, "t b (fs c) ... -> t b fs c ...", fs=self.frame_stack)

        if weight is not None:

            expand_dim = len(loss.shape) - len(weight.shape) - 1

            weight = rearrange(

                weight,

                "(t fs) b ... -> t b fs ..." + " 1" * expand_dim,

                fs=self.frame_stack,

            )

            loss = loss * weight



        return loss.mean()



    def _preprocess_batch(self, batch):

        xs = batch[0]

        batch_size, n_frames = xs.shape[:2]



        if n_frames % self.frame_stack != 0:

            raise ValueError("Number of frames must be divisible by frame stack size")

        if self.context_frames % self.frame_stack != 0:

            raise ValueError("Number of context frames must be divisible by frame stack size")



        masks = torch.ones(n_frames, batch_size).to(xs.device)

        n_frames = n_frames // self.frame_stack



        if self.action_cond_dim:

            conditions = batch[1]

            conditions = torch.cat([torch.zeros_like(conditions[:, :1]), conditions[:, 1:]], 1)

            conditions = rearrange(conditions, "b (t fs) d -> t b (fs d)", fs=self.frame_stack).contiguous()



            # f, _, _ = conditions.shape

            # predefined_1 = torch.tensor([0,0,0,1]).to(conditions.device)

            # predefined_2 = torch.tensor([0,0,1,0]).to(conditions.device)

            # conditions[:f//2] = predefined_1

            # conditions[f//2:] = predefined_2

        else:

            conditions = [None for _ in range(n_frames)]



        xs = self._normalize_x(xs)

        xs = rearrange(xs, "b (t fs) c ... -> t b (fs c) ...", fs=self.frame_stack).contiguous()



        return xs, conditions, masks



    def _normalize_x(self, xs):

        shape = [1] * (xs.ndim - self.data_mean.ndim) + list(self.data_mean.shape)

        mean = self.data_mean.reshape(shape)

        std = self.data_std.reshape(shape)

        return (xs - mean) / std



    def _unnormalize_x(self, xs):

        shape = [1] * (xs.ndim - self.data_mean.ndim) + list(self.data_mean.shape)

        mean = self.data_mean.reshape(shape)

        std = self.data_std.reshape(shape)

        return xs * std + mean



    def _unstack_and_unnormalize(self, xs):

        xs = rearrange(xs, "t b (fs c) ... -> (t fs) b c ...", fs=self.frame_stack)

        return self._unnormalize_x(xs)







——————————————————————————————————————————————————

# df_video.py


import os
import random
import math
import numpy as np
import torch
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from torchvision.transforms import InterpolationMode
from PIL import Image
from packaging import version as pver
from einops import rearrange
from tqdm import tqdm
from omegaconf import DictConfig
from lightning.pytorch.utilities.types import STEP_OUTPUT
from algorithms.common.metrics import (
    LearnedPerceptualImagePatchSimilarity,
)
from utils.logging_utils import log_video, get_validation_metrics_for_videos
from .df_base import DiffusionForcingBase
from .models.vae import VAE_models
from .models.diffusion import Diffusion
from .models.pose_prediction import PosePredictionNet
from .dinov3_feature_extractor import DINOv3FeatureExtractor

import glob

# Utility Functions
def euler_to_rotation_matrix(pitch, yaw):
    """
    Convert pitch and yaw angles (in radians) to a 3x3 rotation matrix.
    Supports batch input.

    Args:
        pitch (torch.Tensor): Pitch angles in radians.
        yaw (torch.Tensor): Yaw angles in radians.

    Returns:
        torch.Tensor: Rotation matrix of shape (batch_size, 3, 3).
    """
    cos_pitch, sin_pitch = torch.cos(pitch), torch.sin(pitch)
    cos_yaw, sin_yaw = torch.cos(yaw), torch.sin(yaw)

    R_pitch = torch.stack([
        torch.ones_like(pitch), torch.zeros_like(pitch), torch.zeros_like(pitch),
        torch.zeros_like(pitch), cos_pitch, -sin_pitch,
        torch.zeros_like(pitch), sin_pitch, cos_pitch
    ], dim=-1).reshape(-1, 3, 3)

    R_yaw = torch.stack([
        cos_yaw, torch.zeros_like(yaw), sin_yaw,
        torch.zeros_like(yaw), torch.ones_like(yaw), torch.zeros_like(yaw),
        -sin_yaw, torch.zeros_like(yaw), cos_yaw
    ], dim=-1).reshape(-1, 3, 3)

    return torch.matmul(R_yaw, R_pitch)


def euler_to_camera_to_world_matrix(pose):
    """
    Convert (x, y, z, pitch, yaw) to a 4x4 camera-to-world transformation matrix using torch.
    Supports both (5,) and (f, b, 5) shaped inputs.

    Args:
        pose (torch.Tensor): Pose tensor of shape (5,) or (f, b, 5).

    Returns:
        torch.Tensor: Camera-to-world transformation matrix of shape (4, 4).
    """

    origin_dim = pose.ndim
    if origin_dim == 1:
        pose = pose.unsqueeze(0).unsqueeze(0)  # Convert (5,) -> (1, 1, 5)
    elif origin_dim == 2:
        pose = pose.unsqueeze(0)

    x, y, z, pitch, yaw = pose[..., 0], pose[..., 1], pose[..., 2], pose[..., 3], pose[..., 4]
    pitch, yaw = torch.deg2rad(pitch), torch.deg2rad(yaw)

    # Compute rotation matrix (batch mode)
    R = euler_to_rotation_matrix(pitch, yaw)  # Shape (f*b, 3, 3)

    # Create the 4x4 transformation matrix
    eye = torch.eye(4, dtype=torch.float32, device=pose.device)
    camera_to_world = eye.repeat(R.shape[0], 1, 1)  # Shape (f*b, 4, 4)

    # Assign rotation
    camera_to_world[:, :3, :3] = R

    # Assign translation
    camera_to_world[:, :3, 3] = torch.stack([x.reshape(-1), y.reshape(-1), z.reshape(-1)], dim=-1)

    # Reshape back to (f, b, 4, 4) if needed
    if origin_dim == 3:
        return camera_to_world.view(pose.shape[0], pose.shape[1], 4, 4)
    elif origin_dim == 2:
        return camera_to_world.view(pose.shape[0], 4, 4)
    else:
        return camera_to_world.squeeze(0).squeeze(0)  # Convert (1,1,4,4) -> (4,4)

def is_inside_fov_3d_hv(points, center, center_pitch, center_yaw, fov_half_h, fov_half_v):
    """
    Check whether points are within a given 3D field of view (FOV) 
    with separately defined horizontal and vertical ranges.

    The center view direction is specified by pitch and yaw (in degrees).

    :param points: (N, B, 3) Sample point coordinates
    :param center: (3,) Center coordinates of the FOV
    :param center_pitch: Pitch angle of the center view (in degrees)
    :param center_yaw: Yaw angle of the center view (in degrees)
    :param fov_half_h: Horizontal half-FOV angle (in degrees)
    :param fov_half_v: Vertical half-FOV angle (in degrees)
    :return: Boolean tensor (N, B), indicating whether each point is inside the FOV
    """
    # Compute vectors relative to the center
    vectors = points - center  # shape (N, B, 3)
    x = vectors[..., 0]
    y = vectors[..., 1]
    z = vectors[..., 2]
    
    # Compute horizontal angle (yaw): measured with respect to the z-axis as the forward direction,
    # and the x-axis as left-right, resulting in a range of -180 to 180 degrees.
    azimuth = torch.atan2(x, z) * (180 / math.pi)
    
    # Compute vertical angle (pitch): measured with respect to the horizontal plane,
    # resulting in a range of -90 to 90 degrees.
    elevation = torch.atan2(y, torch.sqrt(x**2 + z**2)) * (180 / math.pi)
    
    # Compute the angular difference from the center view (handling circular angle wrap-around)
    diff_azimuth = (azimuth - center_yaw).abs() % 360
    diff_elevation = (elevation - center_pitch).abs() % 360
    
    # Adjust values greater than 180 degrees to the shorter angular difference
    diff_azimuth = torch.where(diff_azimuth > 180, 360 - diff_azimuth, diff_azimuth)
    diff_elevation = torch.where(diff_elevation > 180, 360 - diff_elevation, diff_elevation)
    
    # Check if both horizontal and vertical angles are within their respective FOV limits
    return (diff_azimuth < fov_half_h) & (diff_elevation < fov_half_v)
    
def generate_points_in_sphere(n_points, radius):
    # Sample three independent uniform distributions
    samples_r = torch.rand(n_points)       # For radius distribution
    samples_phi = torch.rand(n_points)     # For azimuthal angle phi
    samples_u = torch.rand(n_points)       # For polar angle theta

    # Apply cube root to ensure uniform volumetric distribution
    r = radius * torch.pow(samples_r, 1/3)
    # Azimuthal angle phi uniformly distributed in [0, 2π]
    phi = 2 * math.pi * samples_phi
    # Convert u to theta to ensure cos(theta) is uniformly distributed
    theta = torch.acos(1 - 2 * samples_u)

    # Convert spherical coordinates to Cartesian coordinates
    x = r * torch.sin(theta) * torch.cos(phi)
    y = r * torch.sin(theta) * torch.sin(phi)
    z = r * torch.cos(theta)

    points = torch.stack((x, y, z), dim=1)
    return points

def tensor_max_with_number(tensor, number):
    number_tensor = torch.tensor(number, dtype=tensor.dtype, device=tensor.device)
    result = torch.max(tensor, number_tensor)
    return result

def custom_meshgrid(*args):
    # ref: https://pytorch.org/docs/stable/generated/torch.meshgrid.html?highlight=meshgrid#torch.meshgrid
    if pver.parse(torch.__version__) < pver.parse('1.10'):
        return torch.meshgrid(*args)
    else:
        return torch.meshgrid(*args, indexing='ij')
    
def camera_to_world_to_world_to_camera(camera_to_world: torch.Tensor) -> torch.Tensor:
    """
    Convert Camera-to-World matrices to World-to-Camera matrices for a tensor with shape (f, b, 4, 4).

    Args:
        camera_to_world (torch.Tensor): A tensor of shape (f, b, 4, 4), where:
            f = number of frames,
            b = batch size.

    Returns:
        torch.Tensor: A tensor of shape (f, b, 4, 4) representing the World-to-Camera matrices.
    """
    # Ensure input is a 4D tensor
    assert camera_to_world.ndim == 4 and camera_to_world.shape[2:] == (4, 4), \
        "Input must be of shape (f, b, 4, 4)"
    
    # Extract the rotation (R) and translation (T) parts
    R = camera_to_world[:, :, :3, :3]  # Shape: (f, b, 3, 3)
    T = camera_to_world[:, :, :3, 3]   # Shape: (f, b, 3)
    
    # Initialize an identity matrix for the output
    world_to_camera = torch.eye(4, device=camera_to_world.device).unsqueeze(0).unsqueeze(0)
    world_to_camera = world_to_camera.repeat(camera_to_world.size(0), camera_to_world.size(1), 1, 1)  # Shape: (f, b, 4, 4)
    
    # Compute the rotation (transpose of R)
    world_to_camera[:, :, :3, :3] = R.transpose(2, 3)
    
    # Compute the translation (-R^T * T)
    world_to_camera[:, :, :3, 3] = -torch.matmul(R.transpose(2, 3), T.unsqueeze(-1)).squeeze(-1)
    
    return world_to_camera.to(camera_to_world.dtype)

def convert_to_plucker(poses, curr_frame, focal_length, image_width, image_height):

    intrinsic = np.asarray([focal_length * image_width,
                                focal_length * image_height,
                                0.5 * image_width,
                                0.5 * image_height], dtype=np.float32)

    c2ws = get_relative_pose(poses, zero_first_frame_scale=curr_frame)
    c2ws = rearrange(c2ws, "t b m n -> b t m n")

    K = torch.as_tensor(intrinsic, device=poses.device, dtype=poses.dtype).repeat(c2ws.shape[0],c2ws.shape[1],1)  # [B, F, 4]
    plucker_embedding = ray_condition(K, c2ws, image_height, image_width, device=c2ws.device)
    plucker_embedding = rearrange(plucker_embedding, "b t h w d -> t b h w d").contiguous()

    return plucker_embedding


def get_relative_pose(abs_c2ws, zero_first_frame_scale):
    abs_w2cs = camera_to_world_to_world_to_camera(abs_c2ws)
    target_cam_c2w = torch.tensor([
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 0, 1]
    ]).to(abs_c2ws.device).to(abs_c2ws.dtype)
    abs2rel = target_cam_c2w @ abs_w2cs[zero_first_frame_scale]
    ret_poses = [abs2rel @ abs_c2w for abs_c2w in abs_c2ws]
    ret_poses = torch.stack(ret_poses)
    return ret_poses

def ray_condition(K, c2w, H, W, device):
    # c2w: B, V, 4, 4
    # K: B, V, 4

    B = K.shape[0]

    j, i = custom_meshgrid(
        torch.linspace(0, H - 1, H, device=device, dtype=c2w.dtype),
        torch.linspace(0, W - 1, W, device=device, dtype=c2w.dtype),
    )
    i = i.reshape([1, 1, H * W]).expand([B, 1, H * W]) + 0.5  # [B, HxW]
    j = j.reshape([1, 1, H * W]).expand([B, 1, H * W]) + 0.5  # [B, HxW]

    fx, fy, cx, cy = K.chunk(4, dim=-1)  # B,V, 1

    zs = torch.ones_like(i, device=device, dtype=c2w.dtype)  # [B, HxW]
    xs = -(i - cx) / fx * zs
    ys = -(j - cy) / fy * zs 

    zs = zs.expand_as(ys)

    directions = torch.stack((xs, ys, zs), dim=-1)  # B, V, HW, 3
    directions = directions / directions.norm(dim=-1, keepdim=True)  # B, V, HW, 3

    rays_d = directions @ c2w[..., :3, :3].transpose(-1, -2)  # B, V, 3, HW
    rays_o = c2w[..., :3, 3]  # B, V, 3
    rays_o = rays_o[:, :, None].expand_as(rays_d)  # B, V, 3, HW
    # c2w @ dirctions
    rays_dxo = torch.linalg.cross(rays_o, rays_d)
    plucker = torch.cat([rays_dxo, rays_d], dim=-1)
    plucker = plucker.reshape(B, c2w.shape[1], H, W, 6)  # B, V, H, W, 6

    return plucker

def random_transform(tensor):
    """
    Apply the same random translation, rotation, and scaling to all frames in the batch.

    Args:
        tensor (torch.Tensor): Input tensor of shape (F, B, 3, H, W).

    Returns:
        torch.Tensor: Transformed tensor of shape (F, B, 3, H, W).
    """
    if tensor.ndim != 5:
        raise ValueError("Input tensor must have shape (F, B, 3, H, W)")

    F, B, C, H, W = tensor.shape

    # Generate random transformation parameters
    max_translate = 0.2  # Translate up to 20% of width/height
    max_rotate = 30      # Rotate up to 30 degrees
    max_scale = 0.2      # Scale change by up to +/- 20%

    translate_x = random.uniform(-max_translate, max_translate) * W
    translate_y = random.uniform(-max_translate, max_translate) * H
    rotate_angle = random.uniform(-max_rotate, max_rotate)
    scale_factor = 1 + random.uniform(-max_scale, max_scale)

    # Apply the same transformation to all frames and batches

    tensor = tensor.reshape(F*B, C, H, W)
    transformed_tensor = TF.affine(
        tensor,
        angle=rotate_angle,
        translate=(translate_x, translate_y),
        scale=scale_factor,
        shear=(0, 0),
        interpolation=InterpolationMode.BILINEAR,
        fill=0
    )

    transformed_tensor = transformed_tensor.reshape(F, B, C, H, W)
    return transformed_tensor

def save_tensor_as_png(tensor, file_path):
    """
    Save a 3*H*W tensor as a PNG image.

    Args:
        tensor (torch.Tensor): Input tensor of shape (3, H, W).
        file_path (str): Path to save the PNG file.
    """
    if tensor.ndim != 3 or tensor.shape[0] != 3:
        raise ValueError("Input tensor must have shape (3, H, W)")

    # Convert tensor to PIL Image
    image = TF.to_pil_image(tensor)

    # Save image
    image.save(file_path)

class WorldMemMinecraft(DiffusionForcingBase):
    """
    Video generation for MineCraft with memory.
    """

    def __init__(self, cfg: DictConfig):
        """
        Initialize the WorldMemMinecraft class with the given configuration.

        Args:
            cfg (DictConfig): Configuration object.
        """
        self.n_tokens = cfg.n_frames // cfg.frame_stack # number of max tokens for the model
        self.n_frames = cfg.n_frames
        if hasattr(cfg, "n_tokens"):
            self.n_tokens = cfg.n_tokens // cfg.frame_stack
        self.memory_condition_length = cfg.memory_condition_length
        self.pose_cond_dim = getattr(cfg, "pose_cond_dim", 5)

        self.use_plucker = cfg.use_plucker
        self.relative_embedding = cfg.relative_embedding
        self.state_embed_only_on_qk = getattr(cfg, "state_embed_only_on_qk", True)
        self.use_memory_attention = getattr(cfg, "use_memory_attention", True)
        self.add_timestamp_embedding = cfg.add_timestamp_embedding
        self.ref_mode = getattr(cfg, "ref_mode", 'sequential')
        self.log_curve = getattr(cfg, "log_curve", False)
        self.focal_length =  getattr(cfg, "focal_length", 0.35)
        self.log_video = cfg.log_video
        self.self_consistency_eval = getattr(cfg, "self_consistency_eval", False)
        self.next_frame_length = getattr(cfg, "next_frame_length", 1)
        self.require_pose_prediction = getattr(cfg, "require_pose_prediction", False)
        self.condition_index_method = getattr(cfg, "condition_index_method", "fov")

        # for the dinov3 feature extractor
        self.dino_feature_extractor = DINOv3FeatureExtractor(model_name='dinov3_vitl16', device=self.device)
        self.memory_candidate_pool_size = 64  # Hyperparameter N
        self.w_geom = 0.4  # Hyperparameter for geometric score weight
        self.w_sem = 0.6   # Hyperparameter for semantic score weight


        super().__init__(cfg)
            
    def _build_model(self):

        self.diffusion_model = Diffusion(
            reference_length=self.memory_condition_length,
            x_shape=self.x_stacked_shape,
            action_cond_dim=self.action_cond_dim,
            pose_cond_dim=self.pose_cond_dim,
            is_causal=self.causal,
            cfg=self.cfg.diffusion,
            is_dit=True,
            use_plucker=self.use_plucker,
            relative_embedding=self.relative_embedding,
            state_embed_only_on_qk=self.state_embed_only_on_qk,
            use_memory_attention=self.use_memory_attention,
            add_timestamp_embedding=self.add_timestamp_embedding,
            ref_mode=self.ref_mode
        )

        self.validation_lpips_model = LearnedPerceptualImagePatchSimilarity()
        vae = VAE_models["vit-l-20-shallow-encoder"]()
        self.vae = vae.eval()

        if self.require_pose_prediction:
            self.pose_prediction_model = PosePredictionNet()

    def _generate_noise_levels(self, xs: torch.Tensor, masks = None) -> torch.Tensor:
        """
        Generate noise levels for training.
        """
        num_frames, batch_size, *_ = xs.shape
        match self.cfg.noise_level:
            case "random_all":  # entirely random noise levels
                noise_levels = torch.randint(0, self.timesteps, (num_frames, batch_size), device=xs.device)
            case "same":
                noise_levels = torch.randint(0, self.timesteps, (num_frames, batch_size), device=xs.device)
                noise_levels[1:] = noise_levels[0]

        if masks is not None:
            # for frames that are not available, treat as full noise
            discard = torch.all(~rearrange(masks.bool(), "(t fs) b -> t b fs", fs=self.frame_stack), -1)
            noise_levels = torch.where(discard, torch.full_like(noise_levels, self.timesteps - 1), noise_levels)

        return noise_levels

    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:
        """
        Perform a single training step.

        This function processes the input batch,
        encodes the input frames, generates noise levels, and computes the loss using the diffusion model.

        Args:
            batch: Input batch of data containing frames, conditions, poses, etc.
            batch_idx: Index of the current batch.

        Returns:
            dict: A dictionary containing the training loss.
        """
        xs, conditions, pose_conditions, c2w_mat, frame_idx = self._preprocess_batch(batch)

        if self.use_plucker:
            if self.relative_embedding:
                input_pose_condition = []
                frame_idx_list = []
                for i in range(self.n_frames):
                    input_pose_condition.append(
                        convert_to_plucker(
                            torch.cat([c2w_mat[i:i + 1], c2w_mat[-self.memory_condition_length:]]).clone(),
                            0,
                            focal_length=self.focal_length,
                            image_height=xs.shape[-2],image_width=xs.shape[-1]
                        ).to(xs.dtype)
                    )
                    frame_idx_list.append(
                        torch.cat([
                            frame_idx[i:i + 1] - frame_idx[i:i + 1],
                            frame_idx[-self.memory_condition_length:] - frame_idx[i:i + 1]
                        ]).clone()
                    )
                input_pose_condition = torch.cat(input_pose_condition)
                frame_idx_list = torch.cat(frame_idx_list)
            else:
                input_pose_condition = convert_to_plucker(
                    c2w_mat, 0, focal_length=self.focal_length
                ).to(xs.dtype)
                frame_idx_list = frame_idx
        else:
            input_pose_condition = pose_conditions.to(xs.dtype)
            frame_idx_list = None

        xs = self.encode(xs)

        noise_levels = self._generate_noise_levels(xs)

        if self.memory_condition_length:
            noise_levels[-self.memory_condition_length:] = self.diffusion_model.stabilization_level
            conditions[-self.memory_condition_length:] *= 0

        _, loss = self.diffusion_model(
            xs,
            conditions,
            input_pose_condition,
            noise_levels=noise_levels,
            reference_length=self.memory_condition_length,
            frame_idx=frame_idx_list
        )

        if self.memory_condition_length:
            loss = loss[:-self.memory_condition_length]

        loss = self.reweight_loss(loss, None)

        if batch_idx % 20 == 0:
            self.log("training/loss", loss.cpu())

        return {"loss": loss}
    
    def on_validation_epoch_end(self, namespace="validation") -> None:
        if not self.validation_step_outputs:
            return
        
        xs_pred = []
        xs = []
        names_all = []
        for item in self.validation_step_outputs:
            if len(item) == 3:
                pred, gt, names = item
                if names is not None:
                    # flatten list of names for this batch
                    names_all.extend(list(names))
            else:
                pred, gt = item
            xs_pred.append(pred)
            xs.append(gt)

        xs_pred = torch.cat(xs_pred, 1)
        if gt is not None:
            xs = torch.cat(xs, 1)
        else:
            xs = None

        if self.logger and self.log_video:
            # log generated videos separately
            log_video(
                xs_pred,
                None,
                step=None if namespace == "test" else self.global_step,
                namespace=namespace + "_vis_generated",
                names=names_all if len(names_all) == xs_pred.shape[1] else None,
                context_frames=self.context_frames,
                logger=self.logger.experiment,
            )
            # log ground-truth videos separately (if available)
            if xs is not None:
                log_video(
                    xs,
                    None,
                    step=None if namespace == "test" else self.global_step,
                    namespace=namespace + "_vis_ground_truth",
                    names=names_all if len(names_all) == xs_pred.shape[1] else None,
                    context_frames=self.context_frames,
                    logger=self.logger.experiment,
                )

        if xs is not None:
            metric_dict = get_validation_metrics_for_videos(
                xs_pred, xs, 
                lpips_model=self.validation_lpips_model)
            
            self.log_dict(
                {"mse": metric_dict['mse'],
                "psnr": metric_dict['psnr'],
                "lpips": metric_dict['lpips']},
                sync_dist=True
            )

            if self.log_curve:
                psnr_values = metric_dict['frame_wise_psnr'].cpu().tolist()
                frames = list(range(len(psnr_values)))
                line_plot = wandb.plot.line_series(
                    xs = frames,
                    ys = [psnr_values],
                    keys = ["PSNR"],
                    title = "Frame-wise PSNR",
                    xname = "Frame index"
                )

                self.logger.experiment.log({"frame_wise_psnr_plot": line_plot})
      
        elif self.self_consistency_eval:
            metric_dict = get_validation_metrics_for_videos(
                xs_pred[:1],
                xs_pred[-1:],
                lpips_model=self.validation_lpips_model,
            )            
            self.log_dict(
                {"lpips": metric_dict['lpips'],
                "mse": metric_dict['mse'],
                "psnr": metric_dict['psnr']},
                sync_dist=True
            )

        self.validation_step_outputs.clear()

    def _preprocess_batch(self, batch):
        if len(batch) == 5:
            xs, conditions, pose_conditions, frame_index, sample_names = batch
            self._current_sample_names = sample_names
        else:
            xs, conditions, pose_conditions, frame_index = batch
            self._current_sample_names = None

        if self.action_cond_dim:
            conditions = torch.cat([torch.zeros_like(conditions[:, :1]), conditions[:, 1:]], 1)
            conditions = rearrange(conditions, "b t d -> t b d").contiguous()
        else:
            raise NotImplementedError("Only support external cond.")

        pose_conditions = rearrange(pose_conditions, "b t d -> t b d").contiguous()
        c2w_mat = euler_to_camera_to_world_matrix(pose_conditions)
        xs = rearrange(xs, "b t c ... -> t b c ...").contiguous()
        frame_index = rearrange(frame_index, "b t -> t b").contiguous()

        return xs, conditions, pose_conditions, c2w_mat, frame_index
    
    def encode(self, x):
        # vae encoding
        T = x.shape[0]
        H, W = x.shape[-2:]
        scaling_factor = 0.07843137255

        x = rearrange(x, "t b c h w -> (t b) c h w")
        with torch.no_grad():
            x = self.vae.encode(x * 2 - 1).mean * scaling_factor
        x = rearrange(x, "(t b) (h w) c -> t b c h w", t=T, h=H // self.vae.patch_size, w=W // self.vae.patch_size)
        return x

    def decode(self, x):
        total_frames = x.shape[0]
        scaling_factor = 0.07843137255
        x = rearrange(x, "t b c h w -> (t b) (h w) c")
        with torch.no_grad():
            x = (self.vae.decode(x / scaling_factor) + 1) / 2
        x = rearrange(x, "(t b) c h w-> t b c h w", t=total_frames)
        return x

    def _generate_condition_indices_mc_fov(self, curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, horizon):
        """
        Generate indices for condition similarity based on the current frame and pose conditions.
        """
        if curr_frame < memory_condition_length:
            random_idx = [i for i in range(curr_frame)] + [0] * (memory_condition_length - curr_frame)
            random_idx = np.repeat(np.array(random_idx)[:, None], xs_pred.shape[1], -1)
        else:
            # Generate points in a sphere and filter based on field of view
            num_samples = 10000
            radius = 30
            points = generate_points_in_sphere(num_samples, radius).to(pose_conditions.device)
            points = points[:, None].repeat(1, pose_conditions.shape[1], 1)
            points += pose_conditions[curr_frame, :, :3][None]
            fov_half_h = torch.tensor(105 / 2, device=pose_conditions.device)
            fov_half_v = torch.tensor(75 / 2, device=pose_conditions.device)

            # in_fov1 = is_inside_fov_3d_hv(
            #     points, pose_conditions[curr_frame, :, :3],
            #     pose_conditions[curr_frame, :, -2], pose_conditions[curr_frame, :, -1],
            #     fov_half_h, fov_half_v
            # )

            in_fov1 = torch.stack([
                is_inside_fov_3d_hv(points, pc[:, :3], pc[:, -2], pc[:, -1], fov_half_h, fov_half_v)
                for pc in pose_conditions[curr_frame:curr_frame+horizon]
            ])

            in_fov1 = torch.sum(in_fov1, 0) > 0

            # Compute overlap ratios and select indices
            in_fov_list = torch.stack([
                is_inside_fov_3d_hv(points, pc[:, :3], pc[:, -2], pc[:, -1], fov_half_h, fov_half_v)
                for pc in pose_conditions[:curr_frame]
            ])

            random_idx = []
            for _ in range(memory_condition_length):
                overlap_ratio = ((in_fov1.bool() & in_fov_list).sum(1)) / in_fov1.sum()
                
                confidence = overlap_ratio + (curr_frame - frame_idx[:curr_frame]) / curr_frame * (-0.2)

                if len(random_idx) > 0:
                    confidence[torch.cat(random_idx)] = -1e10
                _, r_idx = torch.topk(confidence, k=1, dim=0)
                random_idx.append(r_idx[0])

                # choice 1: directly remove overlapping region
                occupied_mask = in_fov_list[r_idx[0, range(in_fov1.shape[-1])], :, range(in_fov1.shape[-1])].permute(1,0)
                in_fov1 = in_fov1 & ~occupied_mask

                # choice 2: apply similarity filter 
                # cos_sim = F.cosine_similarity(xs_pred.to(r_idx.device)[r_idx[:, range(in_fov1.shape[1])], 
                #     range(in_fov1.shape[1])], xs_pred.to(r_idx.device)[:curr_frame], dim=2)
                # cos_sim = cos_sim.mean((-2,-1))

                # mask_sim = cos_sim>0.9
                # in_fov_list = in_fov_list & ~mask_sim[:,None].to(in_fov_list.device)

            random_idx = torch.stack(random_idx).cpu()

        return random_idx

    def _generate_condition_indices_knn(self, curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, horizon):
        """
        MODIFIED: Generate indices for memory frames based on pose similarity (K-Nearest Neighbors).
        This version replaces the original FOV overlap calculation with a faster distance-based search.
        It selects the K memory frames with the smallest pose distance to the current frame.
        """
        # Handles the initial frames when the memory bank is not yet full.
        if curr_frame < memory_condition_length:
            # Pad with the first frame if not enough unique frames are available.
            random_idx = [i for i in range(curr_frame)] + [0] * (memory_condition_length - curr_frame)
            # Repeat indices for each item in the batch.
            random_idx = np.repeat(np.array(random_idx)[:, None], xs_pred.shape[1], -1)
            # Convert to a PyTorch tensor on the CPU, which is the expected output format.
            random_idx = torch.from_numpy(random_idx)
        else:
            # --- KNN-based Retrieval Logic ---
            
            # Get the current pose and all past poses from the memory bank.
            # `pose_conditions` has shape [total_frames, batch_size, 5]
            current_pose = pose_conditions[curr_frame]      # Shape: [batch_size, 5]
            memory_poses = pose_conditions[:curr_frame]      # Shape: [curr_frame, batch_size, 5]

            # --- 1. Calculate Positional Distance (Euclidean) ---
            # Unsqueeze adds a dimension for broadcasting against the memory poses.
            current_pos = current_pose[:, :3].unsqueeze(0)   # Shape: [1, batch_size, 3]
            memory_pos = memory_poses[:, :, :3]            # Shape: [curr_frame, batch_size, 3]
            
            # `torch.linalg.norm` computes the Euclidean distance along the last dimension.
            position_distance = torch.linalg.norm(current_pos - memory_pos, dim=-1) # Shape: [curr_frame, batch_size]

            # --- 2. Calculate Orientation Distance (Angular) ---
            # This weight is a tunable hyperparameter to balance position vs. orientation.
            orientation_weight = 0.5
            current_orient = current_pose[:, 3:].unsqueeze(0) # Shape: [1, batch_size, 2]
            memory_orient = memory_poses[:, :, 3:]          # Shape: [curr_frame, batch_size, 2]

            # Calculate the shortest angle difference, handling the 360-degree wraparound.
            orient_diff = torch.abs(current_orient - memory_orient)
            orient_diff = torch.min(orient_diff, 360.0 - orient_diff)
            orientation_distance = torch.linalg.norm(orient_diff, dim=-1) # Shape: [curr_frame, batch_size]
            
            # --- 3. Combine Distances into a Single Score ---
            # A lower score indicates a closer pose.
            pose_score = position_distance + orientation_weight * orientation_distance

            # --- 4. Apply Time Penalty (from original paper) ---
            # This discourages retrieving very old frames unless they are an exceptionally good match.
            time_penalty_weight = 0.2
            # `frame_idx` has shape [total_frames, batch_size]
            time_difference = frame_idx[curr_frame].unsqueeze(0) - frame_idx[:curr_frame]
            # Add a small epsilon to avoid division by zero if curr_frame is 0.
            time_penalty = (time_difference.float() / (curr_frame + 1e-6)) * time_penalty_weight

            # --- 5. Calculate Final Score ---
            # Combine pose distance and time penalty. A lower score is better.
            final_score = pose_score + time_penalty

            # --- 6. Select the Top K Indices ---
            # We find the `k` smallest scores for each item in the batch along the frame dimension (dim=0).
            _, top_indices = torch.topk(
                final_score, 
                k=memory_condition_length, 
                dim=0, 
                largest=False
            ) # Shape: [memory_condition_length, batch_size]

            random_idx = top_indices.cpu()

        return random_idx

    def _prepare_conditions(self, 
                            start_frame, curr_frame, horizon, conditions, 
                            pose_conditions, c2w_mat, frame_idx, random_idx,
                            image_width, image_height):
        """
        Prepare input conditions and pose conditions for sampling.
        """

        padding = torch.zeros((len(random_idx),) + conditions.shape[1:], device=conditions.device, dtype=conditions.dtype)
        input_condition = torch.cat([conditions[start_frame:curr_frame + horizon], padding], dim=0)

        batch_size = conditions.shape[1]

        if self.use_plucker:
            if self.relative_embedding:
                frame_idx_list = []
                input_pose_condition = []
                for i in range(start_frame, curr_frame + horizon):
                    input_pose_condition.append(convert_to_plucker(torch.cat([c2w_mat[i:i+1],c2w_mat[random_idx[:,range(batch_size)], range(batch_size)]]).clone(), 0, focal_length=self.focal_length,
                                                image_width=image_width, image_height=image_height).to(conditions.dtype))
                    frame_idx_list.append(torch.cat([frame_idx[i:i+1]-frame_idx[i:i+1], frame_idx[random_idx[:,range(batch_size)], range(batch_size)]-frame_idx[i:i+1]]))
                input_pose_condition = torch.cat(input_pose_condition)
                frame_idx_list = torch.cat(frame_idx_list)

            else:
                input_pose_condition = torch.cat([c2w_mat[start_frame : curr_frame + horizon], c2w_mat[random_idx[:,range(batch_size)], range(batch_size)]], dim=0).clone()
                input_pose_condition = convert_to_plucker(input_pose_condition, 0, focal_length=self.focal_length)
                frame_idx_list = None
        else:
            input_pose_condition = torch.cat([pose_conditions[start_frame : curr_frame + horizon], pose_conditions[random_idx[:,range(batch_size)], range(batch_size)]], dim=0).clone()
            frame_idx_list = None

        return input_condition, input_pose_condition, frame_idx_list

    def _prepare_noise_levels(self, scheduling_matrix, m, curr_frame, batch_size, memory_condition_length):
        """
        Prepare noise levels for the current sampling step.
        """
        from_noise_levels = np.concatenate((np.zeros((curr_frame,), dtype=np.int64), scheduling_matrix[m]))[:, None].repeat(batch_size, axis=1)
        to_noise_levels = np.concatenate((np.zeros((curr_frame,), dtype=np.int64), scheduling_matrix[m + 1]))[:, None].repeat(batch_size, axis=1)
        if memory_condition_length:
            from_noise_levels = np.concatenate([from_noise_levels, np.zeros((memory_condition_length, from_noise_levels.shape[-1]), dtype=np.int32)], axis=0)
            to_noise_levels = np.concatenate([to_noise_levels, np.zeros((memory_condition_length, from_noise_levels.shape[-1]), dtype=np.int32)], axis=0)
        from_noise_levels = torch.from_numpy(from_noise_levels).to(self.device)
        to_noise_levels = torch.from_numpy(to_noise_levels).to(self.device)
        return from_noise_levels, to_noise_levels

    def validation_step(self, batch, batch_idx, namespace="validation") -> STEP_OUTPUT:
        """
        Perform a single validation step.

        This function processes the input batch, encodes frames, generates predictions using a sliding window approach,
        and handles condition similarity logic for sampling. The results are decoded and stored for evaluation.

        Args:
            batch: Input batch of data containing frames, conditions, poses, etc.
            batch_idx: Index of the current batch.
            namespace: Namespace for logging (default: "validation").

        Returns:
            None: Appends the predicted and ground truth frames to `self.validation_step_outputs`.
        """
        # Preprocess the input batch
        memory_condition_length = self.memory_condition_length
        xs_raw, conditions, pose_conditions, c2w_mat, frame_idx = self._preprocess_batch(batch)
        names = getattr(self, "_current_sample_names", None)


        # Encode frames in chunks if necessary
        total_frame = xs_raw.shape[0]
        if total_frame > 10:
            xs = torch.cat([
                self.encode(xs_raw[int(total_frame * i / 10):int(total_frame * (i + 1) / 10)]).cpu()
                for i in range(10)
            ])
        else:
            xs = self.encode(xs_raw).cpu()

        n_frames, batch_size, *_ = xs.shape
        curr_frame = 0

        # Initialize context frames
        n_context_frames = self.context_frames // self.frame_stack
        xs_pred = xs[:n_context_frames].clone()
        curr_frame += n_context_frames

        # Progress bar for sampling
        pbar = tqdm(total=n_frames, initial=curr_frame, desc="Sampling")

        while curr_frame < n_frames:
            # Determine the horizon for the current chunk
            horizon = min(n_frames - curr_frame, self.chunk_size) if self.chunk_size > 0 else n_frames - curr_frame
            assert horizon <= self.n_tokens, "Horizon exceeds the number of tokens."

            # Generate scheduling matrix and initialize noise
            scheduling_matrix = self._generate_scheduling_matrix(horizon)
            chunk = torch.randn((horizon, batch_size, *xs_pred.shape[2:]))
            chunk = torch.clamp(chunk, -self.clip_noise, self.clip_noise).to(xs_pred.device)
            xs_pred = torch.cat([xs_pred, chunk], 0)

            # Sliding window: only input the last `n_tokens` frames
            start_frame = max(0, curr_frame + horizon - self.n_tokens)
            pbar.set_postfix({"start": start_frame, "end": curr_frame + horizon})

            # Handle condition similarity logic
            if memory_condition_length:
                if self.condition_index_method.lower() == "knn":
                    random_idx = self._generate_condition_indices_knn(
                        curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, horizon
                    )
                else:
                    random_idx = self._generate_condition_indices_mc_fov(
                        curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, horizon
                    )

                xs_pred = torch.cat([xs_pred, xs_pred[random_idx[:, range(xs_pred.shape[1])], range(xs_pred.shape[1])].clone()], 0)

            # Prepare input conditions and pose conditions
            input_condition, input_pose_condition, frame_idx_list = self._prepare_conditions(
                start_frame, curr_frame, horizon, conditions, pose_conditions, c2w_mat, frame_idx, random_idx,
                image_width=xs_raw.shape[-1], image_height=xs_raw.shape[-2]
            )

            # Perform sampling for each step in the scheduling matrix
            for m in range(scheduling_matrix.shape[0] - 1):
                from_noise_levels, to_noise_levels = self._prepare_noise_levels(
                    scheduling_matrix, m, curr_frame, batch_size, memory_condition_length
                )

                xs_pred[start_frame:] = self.diffusion_model.sample_step(
                    xs_pred[start_frame:].to(input_condition.device),
                    input_condition,
                    input_pose_condition,
                    from_noise_levels[start_frame:],
                    to_noise_levels[start_frame:],
                    current_frame=curr_frame,
                    mode="validation",
                    reference_length=memory_condition_length,
                    frame_idx=frame_idx_list
                ).cpu()

            # Remove condition similarity frames if applicable
            if memory_condition_length:
                xs_pred = xs_pred[:-memory_condition_length]

            curr_frame += horizon
            pbar.update(horizon)

        # Decode predictions and ground truth
        xs_pred = self.decode(xs_pred[n_context_frames:].to(conditions.device))
        xs_decode = self.decode(xs[n_context_frames:].to(conditions.device))

        # Store results for evaluation
        self.validation_step_outputs.append((xs_pred, xs_decode, names))
        return

    @torch.no_grad()
    def interactive(self, first_frame, new_actions, first_pose, device,
                    memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx):
    
        memory_condition_length = self.memory_condition_length

        if memory_latent_frames is None:
            first_frame = torch.from_numpy(first_frame)
            new_actions = torch.from_numpy(new_actions)
            first_pose = torch.from_numpy(first_pose)
            first_frame_encode = self.encode(first_frame[None, None].to(device))
            memory_latent_frames = first_frame_encode.cpu()
            memory_actions = new_actions[None, None].to(device)
            memory_poses = first_pose[None, None].to(device)
            new_c2w_mat = euler_to_camera_to_world_matrix(first_pose)
            memory_c2w = new_c2w_mat[None, None].to(device)
            memory_frame_idx = torch.tensor([[0]]).to(device)
            return first_frame.cpu().numpy(), memory_latent_frames.cpu().numpy(), memory_actions.cpu().numpy(), memory_poses.cpu().numpy(), memory_c2w.cpu().numpy(), memory_frame_idx.cpu().numpy()
        else:
            memory_latent_frames = torch.from_numpy(memory_latent_frames)
            memory_actions = torch.from_numpy(memory_actions).to(device)
            memory_poses = torch.from_numpy(memory_poses).to(device)
            memory_c2w = torch.from_numpy(memory_c2w).to(device)
            memory_frame_idx = torch.from_numpy(memory_frame_idx).to(device)
            new_actions = new_actions.to(device)

        curr_frame = 0
        batch_size = 1
        horizon = self.next_frame_length
        n_frames = curr_frame + horizon
        # context
        n_context_frames = len(memory_latent_frames)
        xs_pred = memory_latent_frames[:n_context_frames].clone()
        curr_frame += n_context_frames

        pbar = tqdm(total=n_frames, initial=curr_frame, desc="Sampling")

        new_pose_condition_list = []
        last_frame = xs_pred[-1].clone()
        last_pose_condition = memory_poses[-1].clone()
        curr_actions = new_actions.clone()
        for hi in range(len(new_actions)):
            last_pose_condition[:,3:] = last_pose_condition[:,3:] // 15
            new_pose_condition_offset = self.pose_prediction_model(last_frame.to(device), curr_actions[None, hi], last_pose_condition)
            new_pose_condition_offset[:,3:] = torch.round(new_pose_condition_offset[:,3:])
            new_pose_condition = last_pose_condition + new_pose_condition_offset
            new_pose_condition[:,3:] = new_pose_condition[:,3:] * 15
            new_pose_condition[:,3:] %= 360
            last_pose_condition = new_pose_condition.clone()
            new_pose_condition_list.append(new_pose_condition[None])
        new_pose_condition_list = torch.cat(new_pose_condition_list, 0)
        
        ai = 0
        while ai < len(new_actions):
            next_horizon = min(horizon, len(new_actions) - ai)
            last_frame = xs_pred[-1].clone()
            curr_actions = new_actions[ai:ai+next_horizon].clone()

            new_pose_condition = new_pose_condition_list[ai:ai+next_horizon].clone()

            new_c2w_mat = euler_to_camera_to_world_matrix(new_pose_condition)
            memory_poses = torch.cat([memory_poses, new_pose_condition])
            memory_actions = torch.cat([memory_actions, curr_actions[:, None]])
            memory_c2w = torch.cat([memory_c2w, new_c2w_mat])
            new_indices = memory_frame_idx[-1,0] + torch.arange(next_horizon, device=memory_frame_idx.device) + 1

            memory_frame_idx = torch.cat([memory_frame_idx, new_indices[:, None]])

            conditions = memory_actions.clone()
            pose_conditions = memory_poses.clone()
            c2w_mat = memory_c2w .clone()
            frame_idx = memory_frame_idx.clone()

            # generation on frame
            scheduling_matrix = self._generate_scheduling_matrix(next_horizon)
            chunk = torch.randn((next_horizon, batch_size, *xs_pred.shape[2:])).to(xs_pred.device)
            chunk = torch.clamp(chunk, -self.clip_noise, self.clip_noise)

            xs_pred = torch.cat([xs_pred, chunk], 0)

            # sliding window: only input the last n_tokens frames
            start_frame = max(0, curr_frame - self.n_tokens)

            pbar.set_postfix(
                {
                    "start": start_frame,
                    "end": curr_frame + next_horizon,
                }
            )

            # Handle condition similarity logic
            if memory_condition_length:
                if self.condition_index_method.lower() == "knn":
                    random_idx = self._generate_condition_indices_knn(
                        curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, next_horizon
                    )
                else:
                    random_idx = self._generate_condition_indices_mc_fov(
                        curr_frame, memory_condition_length, xs_pred, pose_conditions, frame_idx, next_horizon
                    )
                
                # random_idx = np.unique(random_idx)[:, None]
                # memory_condition_length = len(random_idx)
                xs_pred = torch.cat([xs_pred, xs_pred[random_idx[:, range(xs_pred.shape[1])], range(xs_pred.shape[1])].clone()], 0)

            # Prepare input conditions and pose conditions
            input_condition, input_pose_condition, frame_idx_list = self._prepare_conditions(
                start_frame, curr_frame, next_horizon, conditions, pose_conditions, c2w_mat, frame_idx, random_idx,
                image_width=first_frame.shape[-1], image_height=first_frame.shape[-2]
            )

            # Perform sampling for each step in the scheduling matrix
            for m in range(scheduling_matrix.shape[0] - 1):
                from_noise_levels, to_noise_levels = self._prepare_noise_levels(
                    scheduling_matrix, m, curr_frame, batch_size, memory_condition_length
                )

                xs_pred[start_frame:] = self.diffusion_model.sample_step(
                    xs_pred[start_frame:].to(input_condition.device),
                    input_condition,
                    input_pose_condition,
                    from_noise_levels[start_frame:],
                    to_noise_levels[start_frame:],
                    current_frame=curr_frame,
                    mode="validation",
                    reference_length=memory_condition_length,
                    frame_idx=frame_idx_list
                ).cpu()


            if memory_condition_length:
                xs_pred = xs_pred[:-memory_condition_length]

            curr_frame += next_horizon
            pbar.update(next_horizon)
            ai += next_horizon

        memory_latent_frames = torch.cat([memory_latent_frames, xs_pred[n_context_frames:]])
        xs_pred = self.decode(xs_pred[n_context_frames:].to(device)).cpu()

        return xs_pred.cpu().numpy(), memory_latent_frames.cpu().numpy(), memory_actions.cpu().numpy(), \
            memory_poses.cpu().numpy(), memory_c2w.cpu().numpy(), memory_frame_idx.cpu().numpy()


——————————————————————————————————————————————————

# dinov3_feature_extractor.py

import torch
import torch.nn as nn
from typing import Dict

class DINOv3FeatureExtractor:
    """
    A wrapper class for loading a frozen DINOv3 model and extracting dense patch features.
    This class ensures the model is loaded only once and operates in an efficient,
    inference-only mode.
    """
    def __init__(self, model_name: str = 'dinov3_vitl16', device: str = 'cuda'):
        """
        Initializes the feature extractor.

        Args:
            model_name (str): The name of the DINOv3 model from torch.hub.
            device (str): The compute device ('cuda' or 'cpu').
        """
        self.device = device
        self.model = self._load_and_freeze_model(model_name)
        
    def _load_and_freeze_model(self, model_name: str) -> nn.Module:
        """
        Loads the specified DINOv3 model, sets it to evaluation mode, and freezes
        all its parameters.
        """
        try:
            model = torch.hub.load('facebookresearch/dinov3', model_name, pretrained=True)
            model.eval()
            model.to(self.device)
            for param in model.parameters():
                param.requires_grad = False
            print(f"Successfully loaded and froze DINOv3 model '{model_name}' on {self.device}.")
            return model
        except Exception as e:
            print(f"Error loading DINOv3 model: {e}")
            raise

    def extract_patch_features(self, image_batch: torch.Tensor) -> torch.Tensor:
        """
        Extracts dense patch features from a batch of images.

        Args:
            image_batch (torch.Tensor): A batch of preprocessed images with shape
                                       , already on the correct device.

        Returns:
            torch.Tensor: A tensor of patch features with shape 
                         .
        """
        if image_batch.device!= self.device:
            image_batch = image_batch.to(self.device)

        with torch.inference_mode():
            # The model's forward pass returns a dictionary of feature types
            features_dict = self.model.forward_features(image_batch)
            
            # We are interested in the final, normalized patch tokens
            patch_features = features_dict.get('x_norm_patchtokens')
            
            if patch_features is None:
                raise KeyError("Could not find 'x_norm_patchtokens' in model output. "
                               "Available keys: " + str(features_dict.keys()))
                               
        return patch_features



——————————————————————————————————————————————————

# pose_prediction.py

from omegaconf import DictConfig

import torch

from lightning.pytorch.utilities.types import STEP_OUTPUT

from algorithms.common.metrics import (

    FrechetInceptionDistance,

    LearnedPerceptualImagePatchSimilarity,

    FrechetVideoDistance,

)

from .df_base import DiffusionForcingBase

from utils.logging_utils import log_video, get_validation_metrics_for_videos

from .models.vae import VAE_models

from .models.dit import DiT_models

from einops import rearrange

from torch import autocast

import numpy as np

from tqdm import tqdm

import torch.nn.functional as F

from .models.pose_prediction import PosePredictionNet

import torchvision.transforms.functional as TF

import random

from torchvision.transforms import InterpolationMode

from PIL import Image

import math

from packaging import version as pver

import torch.distributed as dist

import matplotlib.pyplot as plt



import torch

import math

import wandb



import torch.nn as nn

from algorithms.common.base_pytorch_algo import BasePytorchAlgo



class PosePrediction(BasePytorchAlgo):



    def __init__(self, cfg: DictConfig):



        super().__init__(cfg)



    def _build_model(self):

        self.pose_prediction_model = PosePredictionNet()

        vae = VAE_models["vit-l-20-shallow-encoder"]()

        self.vae = vae.eval()



    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:

        xs, conditions, pose_conditions= batch

        pose_conditions[:,:,3:] = pose_conditions[:,:,3:] // 15

        xs = self.encode(xs)



        b,f,c,h,w = xs.shape

        xs = xs[:,:-1].reshape(-1, c, h, w)

        conditions = conditions[:,1:].reshape(-1, 25)

        offset_gt = pose_conditions[:,1:] - pose_conditions[:,:-1]

        pose_conditions = pose_conditions[:,:-1].reshape(-1, 5)

        offset_gt = offset_gt.reshape(-1, 5)

        offset_gt[:, 3][offset_gt[:, 3]==23] = -1

        offset_gt[:, 3][offset_gt[:, 3]==-23] = 1

        offset_gt[:, 4][offset_gt[:, 4]==23] = -1

        offset_gt[:, 4][offset_gt[:, 4]==-23] = 1



        offset_pred = self.pose_prediction_model(xs, conditions, pose_conditions)

        criterion = nn.MSELoss()

        loss = criterion(offset_pred, offset_gt)

        if batch_idx % 200 == 0:

            self.log("training/loss", loss.cpu())

        output_dict = {

            "loss": loss}

        return output_dict



    def encode(self, x):

        # vae encoding

        B = x.shape[1]

        T = x.shape[0]

        H, W = x.shape[-2:]

        scaling_factor = 0.07843137255



        x = rearrange(x, "t b c h w -> (t b) c h w")

        with torch.no_grad():

            with autocast("cuda", dtype=torch.half):

                x = self.vae.encode(x * 2 - 1).mean * scaling_factor

        x = rearrange(x, "(t b) (h w) c -> t b c h w", t=T, h=H // self.vae.patch_size, w=W // self.vae.patch_size)

        # x = x[:, :n_prompt_frames]

        return x



    def decode(self, x):

        total_frames = x.shape[0]

        scaling_factor = 0.07843137255

        x = rearrange(x, "t b c h w -> (t b) (h w) c")

        with torch.no_grad():

            with autocast("cuda", dtype=torch.half):

                x = (self.vae.decode(x / scaling_factor) + 1) / 2



        x = rearrange(x, "(t b) c h w-> t b c h w", t=total_frames)

        return x



    def validation_step(self, batch, batch_idx, namespace="validation") -> STEP_OUTPUT:

        xs, conditions, pose_conditions= batch

        pose_conditions[:,:,3:] = pose_conditions[:,:,3:] // 15

        xs = self.encode(xs)



        b,f,c,h,w = xs.shape

        xs = xs[:,:-1].reshape(-1, c, h, w)

        conditions = conditions[:,1:].reshape(-1, 25)

        offset_gt = pose_conditions[:,1:] - pose_conditions[:,:-1]

        pose_conditions = pose_conditions[:,:-1].reshape(-1, 5)

        offset_gt = offset_gt.reshape(-1, 5)

        offset_gt[:, 3][offset_gt[:, 3]==23] = -1

        offset_gt[:, 3][offset_gt[:, 3]==-23] = 1

        offset_gt[:, 4][offset_gt[:, 4]==23] = -1

        offset_gt[:, 4][offset_gt[:, 4]==-23] = 1



        offset_pred = self.pose_prediction_model(xs, conditions, pose_conditions)



        criterion = nn.MSELoss()

        loss = criterion(offset_pred, offset_gt)



        if batch_idx % 200 == 0:

            self.log("validation/loss", loss.cpu())

        output_dict = {

            "loss": loss}

        return



    @torch.no_grad()

    def interactive(self, batch, context_frames, device):

        with torch.cuda.amp.autocast():

            condition_similar_length = self.condition_similar_length

            # xs_raw, conditions, pose_conditions, c2w_mat, masks, frame_idx = self._preprocess_batch(batch)



            first_frame, new_conditions, new_pose_conditions, new_c2w_mat, new_frame_idx = batch



            if self.frames is None:

                first_frame_encode = self.encode(first_frame[None, None].to(device))

                self.frames = first_frame_encode.to(device)

                self.actions = new_conditions[None, None].to(device)

                self.poses = new_pose_conditions[None, None].to(device)

                self.memory_c2w = new_c2w_mat[None, None].to(device)

                self.frame_idx = torch.tensor([[new_frame_idx]]).to(device)

                return first_frame

            else:

                self.actions = torch.cat([self.actions, new_conditions[None, None].to(device)])

                self.poses = torch.cat([self.poses, new_pose_conditions[None, None].to(device)])

                self.memory_c2w = torch.cat([self.memory_c2w, new_c2w_mat[None, None].to(device)])

                self.frame_idx = torch.cat([self.frame_idx, torch.tensor([[new_frame_idx]]).to(device)])



            conditions = self.actions.clone()

            pose_conditions = self.poses.clone()

            c2w_mat = self.memory_c2w .clone()

            frame_idx = self.frame_idx.clone()





            curr_frame = 0

            horizon = 1

            batch_size = 1

            n_frames = curr_frame + horizon

            # context

            n_context_frames = context_frames // self.frame_stack

            xs_pred = self.frames[:n_context_frames].clone()

            curr_frame += n_context_frames



            pbar = tqdm(total=n_frames, initial=curr_frame, desc="Sampling")



            # generation on frame

            scheduling_matrix = self._generate_scheduling_matrix(horizon)

            chunk = torch.randn((horizon, batch_size, *xs_pred.shape[2:])).to(xs_pred.device)

            chunk = torch.clamp(chunk, -self.clip_noise, self.clip_noise)



            xs_pred = torch.cat([xs_pred, chunk], 0)



            # sliding window: only input the last n_tokens frames

            start_frame = max(0, curr_frame + horizon - self.n_tokens)



            pbar.set_postfix(

                {

                    "start": start_frame,

                    "end": curr_frame + horizon,

                }

            )



            if condition_similar_length:



                if curr_frame < condition_similar_length:

                    random_idx = [i for i in range(curr_frame)] + [0] * (condition_similar_length-curr_frame)

                    random_idx = np.repeat(np.array(random_idx)[:,None], xs_pred.shape[1], -1)

                else:

                    num_samples = 10000

                    radius = 30

                    samples = torch.rand((num_samples, 1), device=pose_conditions.device)

                    angles = 2 * np.pi * torch.rand((num_samples,), device=pose_conditions.device)

                    # points = radius * torch.sqrt(samples) * torch.stack((torch.cos(angles), torch.sin(angles)), dim=1)

                    

                    points = generate_points_in_sphere(num_samples, radius).to(pose_conditions.device)

                    points = points[:, None].repeat(1, pose_conditions.shape[1], 1)

                    points += pose_conditions[curr_frame, :, :3][None]

                    fov_half_h = torch.tensor(105/2, device=pose_conditions.device)

                    fov_half_v = torch.tensor(75/2, device=pose_conditions.device)

                    # in_fov1 = is_inside_fov(points, pose_conditions[curr_frame, :, [0, 2]], pose_conditions[curr_frame, :, -1], fov_half)



                    in_fov1 = is_inside_fov_3d_hv(points, pose_conditions[curr_frame, :, :3], 

                        pose_conditions[curr_frame, :, -2], pose_conditions[curr_frame, :, -1],

                        fov_half_h, fov_half_v)



                    in_fov_list = []

                    for pc in pose_conditions[:curr_frame]:

                        in_fov_list.append(is_inside_fov_3d_hv(points, pc[:, :3], pc[:, -2], pc[:, -1],

                                                        fov_half_h, fov_half_v))

                    

                    in_fov_list = torch.stack(in_fov_list)

                    # v3

                    random_idx = []



                    for csl in range(self.condition_similar_length // 2):

                        overlap_ratio = ((in_fov1[None].bool() & in_fov_list).sum(1))/in_fov1.sum()

                        # mask = distance > (in_fov1.bool().sum(0) / 4)

                        #_, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)

                        

                        # if csl > self.condition_similar_length:

                        #   _, r_idx = torch.topk(overlap_ratio, k=1, dim=0)

                        # else:

                        #   _, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)



                        _, r_idx = torch.topk(overlap_ratio, k=1, dim=0)

                        # _, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)



                        # if curr_frame >=93:

                        #     import pdb;pdb.set_trace()



                        # start_time = time.time()

                        cos_sim = F.cosine_similarity(xs_pred.to(r_idx.device)[r_idx[:, range(in_fov1.shape[1])], 

                            range(in_fov1.shape[1])], xs_pred.to(r_idx.device)[:curr_frame], dim=2)

                        cos_sim = cos_sim.mean((-2,-1))



                        mask_sim = cos_sim>0.9

                        in_fov_list = in_fov_list & ~mask_sim[:,None].to(in_fov_list.device)



                        random_idx.append(r_idx)



                    for bi in range(conditions.shape[1]):

                        if len(torch.nonzero(conditions[:,bi,24] == 1))==0:

                            pass

                        else:

                            last_idx = torch.nonzero(conditions[:,bi,24] == 1)[-1]

                            in_fov_list[:last_idx,:,bi] = False



                    for csl in range(self.condition_similar_length // 2):

                        overlap_ratio = ((in_fov1[None].bool() & in_fov_list).sum(1))/in_fov1.sum()

                        # mask = distance > (in_fov1.bool().sum(0) / 4)

                        #_, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)

                        

                        # if csl > self.condition_similar_length:

                        #   _, r_idx = torch.topk(overlap_ratio, k=1, dim=0)

                        # else:

                        #   _, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)



                        _, r_idx = torch.topk(overlap_ratio, k=1, dim=0)

                        # _, r_idx = torch.topk(overlap_ratio / tensor_max_with_number((frame_idx[curr_frame] - frame_idx[:curr_frame]), 10), k=1, dim=0)



                        # if curr_frame >=93:

                        #     import pdb;pdb.set_trace()



                        # start_time = time.time()

                        cos_sim = F.cosine_similarity(xs_pred.to(r_idx.device)[r_idx[:, range(in_fov1.shape[1])], 

                            range(in_fov1.shape[1])], xs_pred.to(r_idx.device)[:curr_frame], dim=2)

                        cos_sim = cos_sim.mean((-2,-1))



                        mask_sim = cos_sim>0.9

                        in_fov_list = in_fov_list & ~mask_sim[:,None].to(in_fov_list.device)



                        random_idx.append(r_idx)

                    

                    random_idx = torch.cat(random_idx).cpu()

                    condition_similar_length = len(random_idx)

                

                xs_pred = torch.cat([xs_pred, xs_pred[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])].clone()], 0)



            if condition_similar_length:

                # import pdb;pdb.set_trace()

                padding = torch.zeros((condition_similar_length,) + conditions.shape[1:], device=conditions.device, dtype=conditions.dtype)

                input_condition = torch.cat([conditions[start_frame : curr_frame + horizon], padding], dim=0)

                if self.pose_cond_dim:

                    # if not self.use_plucker:

                    input_pose_condition = torch.cat([pose_conditions[start_frame : curr_frame + horizon], pose_conditions[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]], dim=0).clone()



                if self.use_plucker:

                    if self.all_zero_frame:

                        frame_idx_list = []

                        input_pose_condition = []

                        for i in range(start_frame, curr_frame + horizon):

                            input_pose_condition.append(convert_to_plucker(torch.cat([c2w_mat[i:i+1],c2w_mat[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]]).clone(), 0, focal_length=self.focal_length, is_old_setting=self.old_setting).to(xs_pred.dtype))

                            frame_idx_list.append(torch.cat([frame_idx[i:i+1]-frame_idx[i:i+1], frame_idx[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]-frame_idx[i:i+1]]))

                        input_pose_condition = torch.cat(input_pose_condition)

                        frame_idx_list = torch.cat(frame_idx_list)



                        # print(frame_idx_list[:,0])

                    else:

                        # print(curr_frame-start_frame)

                        # input_pose_condition = torch.cat([c2w_mat[start_frame : curr_frame + horizon], c2w_mat[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]], dim=0).clone()

                        # import pdb;pdb.set_trace()

                        if self.last_frame_refer:

                            input_pose_condition = torch.cat([c2w_mat[start_frame : curr_frame + horizon], c2w_mat[-1:]], dim=0).clone()

                        else:

                            input_pose_condition = torch.cat([c2w_mat[start_frame : curr_frame + horizon], c2w_mat[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]], dim=0).clone()

                        

                        if self.zero_curr:

                            # print("="*50)

                            input_pose_condition = convert_to_plucker(input_pose_condition, curr_frame-start_frame, focal_length=self.focal_length, is_old_setting=self.old_setting)

                            # input_pose_condition[:curr_frame-start_frame] = input_pose_condition[curr_frame-start_frame:curr_frame-start_frame+1]

                        # input_pose_condition = convert_to_plucker(input_pose_condition, -self.condition_similar_length-1, focal_length=self.focal_length)

                        else:

                            input_pose_condition = convert_to_plucker(input_pose_condition, -condition_similar_length, focal_length=self.focal_length, is_old_setting=self.old_setting)

                        frame_idx_list = None

                else:

                    input_pose_condition = torch.cat([pose_conditions[start_frame : curr_frame + horizon], pose_conditions[random_idx[:,range(xs_pred.shape[1])], range(xs_pred.shape[1])]], dim=0).clone()

                    frame_idx_list = None

            else:

                input_condition = conditions[start_frame : curr_frame + horizon]

                input_pose_condition = None

                frame_idx_list = None

                

            for m in range(scheduling_matrix.shape[0] - 1):

                from_noise_levels = np.concatenate((np.zeros((curr_frame,), dtype=np.int64), scheduling_matrix[m]))[

                    :, None

                ].repeat(batch_size, axis=1)

                to_noise_levels = np.concatenate(

                    (

                        np.zeros((curr_frame,), dtype=np.int64),

                        scheduling_matrix[m + 1],

                    )

                )[

                    :, None

                ].repeat(batch_size, axis=1)



                if condition_similar_length:

                    from_noise_levels = np.concatenate([from_noise_levels, np.zeros((condition_similar_length,from_noise_levels.shape[-1]), dtype=np.int32)], axis=0)

                    to_noise_levels = np.concatenate([to_noise_levels, np.zeros((condition_similar_length,from_noise_levels.shape[-1]), dtype=np.int32)], axis=0)



                from_noise_levels = torch.from_numpy(from_noise_levels).to(self.device)

                to_noise_levels = torch.from_numpy(to_noise_levels).to(self.device)





                if input_pose_condition is not None:

                    input_pose_condition = input_pose_condition.to(xs_pred.dtype)

                

                xs_pred[start_frame:] = self.diffusion_model.sample_step(

                    xs_pred[start_frame:],

                    input_condition,

                    input_pose_condition,

                    from_noise_levels[start_frame:],

                    to_noise_levels[start_frame:],

                    current_frame=curr_frame,

                    mode="validation",

                    reference_length=condition_similar_length,

                    frame_idx=frame_idx_list

                )



                # if curr_frame > 14:

                #     import pdb;pdb.set_trace()



                # if xs_pred_back is not None:

                #     xs_pred = torch.cat([xs_pred[:6], xs_pred_back[6:12], xs_pred[6:]], dim=0)

        

            # import pdb;pdb.set_trace()

            if condition_similar_length: # and curr_frame+1!=n_frames:

                xs_pred = xs_pred[:-condition_similar_length]



            curr_frame += horizon

            pbar.update(horizon)



            self.frames = torch.cat([self.frames, xs_pred[n_context_frames:]])



            xs_pred = self.decode(xs_pred[n_context_frames:])



            return xs_pred[-1,0].cpu()









——————————————————————————————————————————————————

# main.py



"""

This repo is forked from [Boyuan Chen](https://boyuan.space/)'s research 

template [repo](https://github.com/buoyancy99/research-template). 

By its MIT license, you must keep the above sentence in `README.md` 

and the `LICENSE` file to credit the author.



Main file for the project. This will create and run new experiments and load checkpoints from wandb. 

Borrowed part of the code from David Charatan and wandb.

"""



import sys

import subprocess

import time

from pathlib import Path



import hydra

from omegaconf import DictConfig, OmegaConf

from omegaconf.omegaconf import open_dict



from utils.print_utils import cyan

from utils.ckpt_utils import download_latest_checkpoint, is_run_id

from utils.cluster_utils import submit_slurm_job

from utils.distributed_utils import is_rank_zero



def get_latest_checkpoint(checkpoint_folder: Path, pattern: str = '*.ckpt'):

    checkpoint_files = list(checkpoint_folder.glob(pattern))

    if not checkpoint_files:

        return None

    latest_checkpoint = max(checkpoint_files, key=lambda f: f.stat().st_mtime)

    return latest_checkpoint



def run_local(cfg: DictConfig):

    # delay some imports in case they are not needed in non-local envs for submission

    from experiments import build_experiment

    from utils.wandb_utils import OfflineWandbLogger, SpaceEfficientWandbLogger



    # Get yaml names

    hydra_cfg = hydra.core.hydra_config.HydraConfig.get()

    cfg_choice = OmegaConf.to_container(hydra_cfg.runtime.choices)



    with open_dict(cfg):

        if cfg_choice["experiment"] is not None:

            cfg.experiment._name = cfg_choice["experiment"]

        if cfg_choice["dataset"] is not None:

            cfg.dataset._name = cfg_choice["dataset"]

        if cfg_choice["algorithm"] is not None:

            cfg.algorithm._name = cfg_choice["algorithm"]



    # Set up the output directory.

    output_dir = getattr(cfg, "output_dir", None)

    if output_dir is not None:

        OmegaConf.set_readonly(hydra_cfg, False)

        hydra_cfg.runtime.output_dir = output_dir

        OmegaConf.set_readonly(hydra_cfg, True)

        

    output_dir = Path(hydra_cfg.runtime.output_dir)

    

    if is_rank_zero:

        print(cyan(f"Outputs will be saved to:"), output_dir)

        (output_dir.parents[1] / "latest-run").unlink(missing_ok=True)

        (output_dir.parents[1] / "latest-run").symlink_to(output_dir, target_is_directory=True)



    # Set up logging with wandb.

    if cfg.wandb.mode != "disabled":

        # If resuming, merge into the existing run on wandb.

        resume = cfg.get("resume", None)

        name = f"{cfg.name} ({output_dir.parent.name}/{output_dir.name})" if resume is None else None



        if "_on_compute_node" in cfg and cfg.cluster.is_compute_node_offline:

            logger_cls = OfflineWandbLogger

        else:

            logger_cls = SpaceEfficientWandbLogger



        offline = cfg.wandb.mode != "online"

        logger = logger_cls(

            name=name,

            save_dir=str(output_dir),

            offline=offline,

            entity=cfg.wandb.entity,

            project=cfg.wandb.project,

            log_model=False,

            config=OmegaConf.to_container(cfg),

            id=resume,

            resume="auto"

        )



    else:

        logger = None



    # Load ckpt

    resume = cfg.get("resume", None)

    load = cfg.get("load", None)

    checkpoint_path = None

    load_id = None

    if load and not is_run_id(load):

        checkpoint_path = load

    if resume:

        load_id = resume

    elif load and is_run_id(load):

        load_id = load

    else:

        load_id = None



    if load_id:

        checkpoint_path = get_latest_checkpoint(output_dir / "checkpoints")

    

    if checkpoint_path and is_rank_zero:

        print(f"Will load checkpoint from {checkpoint_path}")



    # launch experiment

    experiment = build_experiment(cfg, logger, checkpoint_path)

    for task in cfg.experiment.tasks:

        experiment.exec_task(task)





def run_slurm(cfg: DictConfig):

    python_args = " ".join(sys.argv[1:]) + " +_on_compute_node=True"

    project_root = Path.cwd()

    while not (project_root / ".git").exists():

        project_root = project_root.parent

        if project_root == Path("/"):

            raise Exception("Could not find repo directory!")



    slurm_log_dir = submit_slurm_job(

        cfg,

        python_args,

        project_root,

    )



    if "cluster" in cfg and cfg.cluster.is_compute_node_offline and cfg.wandb.mode == "online":

        print("Job submitted to a compute node without internet. This requires manual syncing on login node.")

        osh_command_dir = project_root / ".wandb_osh_command_dir"



        osh_proc = None

        # if click.confirm("Do you want us to run the sync loop for you?", default=True):

        osh_proc = subprocess.Popen(["wandb-osh", "--command-dir", osh_command_dir])

        print(f"Running wandb-osh in background... PID: {osh_proc.pid}")

        print(f"To kill the sync process, run 'kill {osh_proc.pid}' in the terminal.")

        print(

            f"You can manually start a sync loop later by running the following:",

            cyan(f"wandb-osh --command-dir {osh_command_dir}"),

        )



    print(

        "Once the job gets allocated and starts running, we will print a command below "

        "for you to trace the errors and outputs: (Ctrl + C to exit without waiting)"

    )

    msg = f"tail -f {slurm_log_dir}/* \n"

    try:

        while not list(slurm_log_dir.glob("*.out")) and not list(slurm_log_dir.glob("*.err")):

            time.sleep(1)

        print(cyan("To trace the outputs and errors, run the following command:"), msg)

    except KeyboardInterrupt:

        print("Keyboard interrupt detected. Exiting...")

        print(

            cyan("To trace the outputs and errors, manually wait for the job to start and run the following command:"),

            msg,

        )





@hydra.main(

    version_base=None,

    config_path="configurations",

    config_name="training",

)

def run(cfg: DictConfig):

    if "_on_compute_node" in cfg and cfg.cluster.is_compute_node_offline:

        with open_dict(cfg):

            if cfg.cluster.is_compute_node_offline and cfg.wandb.mode == "online":

                cfg.wandb.mode = "offline"



    if "name" not in cfg:

        raise ValueError("must specify a name for the run with command line argument '+name=[name]'")



    if not cfg.wandb.get("entity", None):

        raise ValueError(

            "must specify wandb entity in 'configurations/config.yaml' or with command line"

            " argument 'wandb.entity=[entity]' \n An entity is your wandb user name or group"

            " name. This is used for logging. If you don't have an wandb account, please signup at https://wandb.ai/"

        )



    if cfg.wandb.project is None:

        cfg.wandb.project = str(Path(__file__).parent.name)



    if "cluster" in cfg and not "_on_compute_node" in cfg:

        print(cyan("Slurm detected, submitting to compute node instead of running locally..."))

        run_slurm(cfg)

    else:

        run_local(cfg)





if __name__ == "__main__":

    run()  # pylint: disable=no-value-for-parameter







——————————————————————————————————————————————————

# app.py



import gradio as gr

import time



import sys

import subprocess

import time

from pathlib import Path



import hydra

from omegaconf import DictConfig, OmegaConf

from omegaconf.omegaconf import open_dict



import numpy as np

import torch

import torchvision.transforms as transforms

import cv2

import subprocess

from PIL import Image

from datetime import datetime

import spaces

from algorithms.worldmem import WorldMemMinecraft

from huggingface_hub import hf_hub_download

import tempfile

import os

import requests

from huggingface_hub import model_info





def is_huggingface_model(path: str) -> bool:

    hf_ckpt = str(path).split('/')

    repo_id = '/'.join(hf_ckpt[:2])

    try:

        model_info(repo_id)

        return True

    except:

        return False

    



torch.set_float32_matmul_precision("high")



def load_custom_checkpoint(algo, checkpoint_path):

    if is_huggingface_model(str(checkpoint_path)):

        hf_ckpt = str(checkpoint_path).split('/')

        repo_id = '/'.join(hf_ckpt[:2])

        file_name = '/'.join(hf_ckpt[2:])

        model_path = hf_hub_download(repo_id=repo_id, 

                            filename=file_name)

        ckpt = torch.load(model_path, map_location=torch.device('cpu'))



        filtered_state_dict = {}

        for k, v in ckpt['state_dict'].items():

            if "frame_timestep_embedder" in k:

                new_k = k.replace("frame_timestep_embedder", "timestamp_embedding")

                filtered_state_dict[new_k] = v

            else:

                filtered_state_dict[k] = v



        algo.load_state_dict(filtered_state_dict, strict=True)

        print("Load: ", model_path)

    else:

        ckpt = torch.load(checkpoint_path, map_location=torch.device('cpu'))



        filtered_state_dict = {}

        for k, v in ckpt['state_dict'].items():

            if "frame_timestep_embedder" in k:

                new_k = k.replace("frame_timestep_embedder", "timestamp_embedding")

                filtered_state_dict[new_k] = v

            else:

                filtered_state_dict[k] = v



        algo.load_state_dict(filtered_state_dict, strict=True)



        # algo.load_state_dict(ckpt['state_dict'], strict=True)      

        print("Load: ", checkpoint_path)  



def download_assets_if_needed():

    ASSETS_URL_BASE = "https://huggingface.co/spaces/yslan/worldmem/resolve/main/assets/examples"

    ASSETS_DIR = "assets/examples"

    ASSETS = ['case1.npz', 'case2.npz', 'case3.npz', 'case4.npz']



    if not os.path.exists(ASSETS_DIR):

        os.makedirs(ASSETS_DIR)

    

    # Download assets if they don't exist (total 4 files)

    for filename in ASSETS:

        filepath = os.path.join(ASSETS_DIR, filename)

        if not os.path.exists(filepath):

            print(f"Downloading {filename}...")

            url = f"{ASSETS_URL_BASE}/{filename}"

            response = requests.get(url)

            if response.status_code == 200:

                with open(filepath, "wb") as f:

                    f.write(response.content)

            else:

                print(f"Failed to download {filename}: {response.status_code}")



def parse_input_to_tensor(input_str):

    """

    Convert an input string into a (sequence_length, 25) tensor, where each row is a one-hot representation 

    of the corresponding action key.



    Args:

        input_str (str): A string consisting of "WASD" characters (e.g., "WASDWS").



    Returns:

        torch.Tensor: A tensor of shape (sequence_length, 25), where each row is a one-hot encoded action.

    """

    # Get the length of the input sequence

    seq_len = len(input_str)

    

    # Initialize a zero tensor of shape (seq_len, 25)

    action_tensor = torch.zeros((seq_len, 25))



    # Iterate through the input string and update the corresponding positions

    for i, char in enumerate(input_str):

        action, value = KEY_TO_ACTION.get(char.upper())  # Convert to uppercase to handle case insensitivity

        if action and action in ACTION_KEYS:

            index = ACTION_KEYS.index(action)

            action_tensor[i, index] = value  # Set the corresponding action index to 1



    return action_tensor



def load_image_as_tensor(image_path: str) -> torch.Tensor:

    """

    Load an image and convert it to a 0-1 normalized tensor.

    

    Args:

        image_path (str): Path to the image file.

    

    Returns:

        torch.Tensor: Image tensor of shape (C, H, W), normalized to [0,1].

    """

    if isinstance(image_path, str):

        image = Image.open(image_path).convert("RGB")  # Ensure it's RGB

    else:

        image = image_path

    transform = transforms.Compose([

        transforms.ToTensor(),  # Converts to tensor and normalizes to [0,1]

    ])

    return transform(image)



def enable_amp(model, precision="16-mixed"):

    original_forward = model.forward



    def amp_forward(*args, **kwargs):

        with torch.autocast("cuda", dtype=torch.float16 if precision == "16-mixed" else torch.bfloat16):

            return original_forward(*args, **kwargs)



    model.forward = amp_forward

    return model



download_assets_if_needed()



ACTION_KEYS = [

    "inventory",

    "ESC",

    "hotbar.1",

    "hotbar.2",

    "hotbar.3",

    "hotbar.4",

    "hotbar.5",

    "hotbar.6",

    "hotbar.7",

    "hotbar.8",

    "hotbar.9",

    "forward",

    "back",

    "left",

    "right",

    "cameraY",

    "cameraX",

    "jump",

    "sneak",

    "sprint",

    "swapHands",

    "attack",

    "use",

    "pickItem",

    "drop",

]



# Mapping of input keys to action names

KEY_TO_ACTION = {

    "Q": ("forward", 1),

    "E": ("back", 1),    

    "W": ("cameraY", -1),

    "S": ("cameraY", 1),

    "A": ("cameraX", -1),

    "D": ("cameraX", 1),

    "U": ("drop", 1),

    "N": ("noop", 1),

    "1": ("hotbar.1", 1),

}



example_images = [

    ["1", "assets/ice_plains.png", "turn rightgo backward→look up→turn left→look down→turn right→go forward→turn left", 20, 3, 8],

    ["2", "assets/place.png", "put item→go backward→put item→go backward→go around", 20, 3, 8],

    ["3", "assets/rain_sunflower_plains.png", "turn right→look up→turn right→look down→turn left→go backward→turn left", 20, 3, 8],

    ["4", "assets/desert.png", "turn 360 degree→turn right→go forward→turn left", 20, 3, 8],

]



video_frames = []

input_history = ""

ICE_PLAINS_IMAGE = "assets/ice_plains.png"

DESERT_IMAGE = "assets/desert.png"

SAVANNA_IMAGE = "assets/savanna.png"

PLAINS_IMAGE = "assets/plans.png"

PLACE_IMAGE = "assets/place.png"

SUNFLOWERS_IMAGE = "assets/sunflower_plains.png"

SUNFLOWERS_RAIN_IMAGE = "assets/rain_sunflower_plains.png"



device = torch.device('cuda')



def save_video(frames, path="output.mp4", fps=10):

    temp_path = path[:-4] + "_temp.mp4"

    h, w, _ = frames[0].shape



    out = cv2.VideoWriter(temp_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))

    for frame in frames:

        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))

    out.release()



    ffmpeg_cmd = [

        "ffmpeg", "-y", "-i", temp_path,

        "-c:v", "libx264", "-crf", "23", "-preset", "medium",

        path

    ]

    subprocess.run(ffmpeg_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    os.remove(temp_path)



cfg = OmegaConf.load("configurations/huggingface.yaml")

worldmem = WorldMemMinecraft(cfg)

load_custom_checkpoint(algo=worldmem.diffusion_model, checkpoint_path=cfg.diffusion_path)

load_custom_checkpoint(algo=worldmem.vae, checkpoint_path=cfg.vae_path)

load_custom_checkpoint(algo=worldmem.pose_prediction_model, checkpoint_path=cfg.pose_predictor_path)

worldmem.to("cuda").eval()

# worldmem = enable_amp(worldmem, precision="16-mixed")



actions = np.zeros((1, 25), dtype=np.float32)

poses = np.zeros((1, 5), dtype=np.float32)







def get_duration_single_image_to_long_video(first_frame, action, first_pose, device, memory_latent_frames, memory_actions, 

                            memory_poses, memory_c2w, memory_frame_idx):

    return 5 * len(action) if memory_actions is not None else 5



@spaces.GPU(duration=get_duration_single_image_to_long_video)

def run_interactive(first_frame, action, first_pose, device, memory_latent_frames, memory_actions, 

                            memory_poses, memory_c2w, memory_frame_idx):

    new_frame, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx = worldmem.interactive(first_frame,

                                    action,

                                    first_pose, 

                                    device=device,

                                    memory_latent_frames=memory_latent_frames,

                                    memory_actions=memory_actions,

                                    memory_poses=memory_poses,

                                    memory_c2w=memory_c2w,

                                    memory_frame_idx=memory_frame_idx)



    return new_frame, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx



def set_denoising_steps(denoising_steps, sampling_timesteps_state):

    worldmem.sampling_timesteps = denoising_steps

    worldmem.diffusion_model.sampling_timesteps = denoising_steps

    sampling_timesteps_state = denoising_steps

    print("set denoising steps to", worldmem.sampling_timesteps)

    return sampling_timesteps_state



def set_context_length(context_length, sampling_context_length_state):

    worldmem.n_tokens = context_length

    sampling_context_length_state = context_length

    print("set context length to", worldmem.n_tokens)

    return sampling_context_length_state



def set_memory_condition_length(memory_condition_length, sampling_memory_condition_length_state):

    worldmem.memory_condition_length = memory_condition_length

    sampling_memory_condition_length_state = memory_condition_length

    print("set memory length to", worldmem.memory_condition_length)

    return sampling_memory_condition_length_state



def set_next_frame_length(next_frame_length, sampling_next_frame_length_state):

    worldmem.next_frame_length = next_frame_length

    sampling_next_frame_length_state = next_frame_length

    print("set next frame length to", worldmem.next_frame_length)

    return sampling_next_frame_length_state



def generate(keys, input_history, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx):

    input_actions = parse_input_to_tensor(keys)



    if memory_latent_frames is None:

        new_frame, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx = run_interactive(video_frames[0],

                                    actions[0],

                                    poses[0],

                                    device=device,

                                    memory_latent_frames=memory_latent_frames,

                                    memory_actions=memory_actions,

                                    memory_poses=memory_poses,

                                    memory_c2w=memory_c2w,

                                    memory_frame_idx=memory_frame_idx)



    new_frame, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx = run_interactive(video_frames[0],

                                    input_actions,

                                    None,

                                    device=device,

                                    memory_latent_frames=memory_latent_frames,

                                    memory_actions=memory_actions,

                                    memory_poses=memory_poses,

                                    memory_c2w=memory_c2w,

                                    memory_frame_idx=memory_frame_idx)



    video_frames = np.concatenate([video_frames, new_frame[:,0]])





    out_video = video_frames.transpose(0,2,3,1).copy()

    out_video = np.clip(out_video, a_min=0.0, a_max=1.0)

    out_video = (out_video * 255).astype(np.uint8)



    last_frame = out_video[-1].copy()

    border_thickness = 2

    out_video[-len(new_frame):, :border_thickness, :, :] = [255, 0, 0]

    out_video[-len(new_frame):, -border_thickness:, :, :] = [255, 0, 0]

    out_video[-len(new_frame):, :, :border_thickness, :] = [255, 0, 0]

    out_video[-len(new_frame):, :, -border_thickness:, :] = [255, 0, 0]



    temporal_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name

    save_video(out_video, temporal_video_path)

    input_history += keys



    

    # now = datetime.now()

    # folder_name = now.strftime("%Y-%m-%d_%H-%M-%S")

    # folder_path = os.path.join("/mnt/xiaozeqi/worldmem/output_material", folder_name)

    # os.makedirs(folder_path, exist_ok=True)

    # data_dict = {

    #     "input_history": input_history,

    #     "video_frames": video_frames,

    #     "memory_latent_frames": memory_latent_frames,

    #     "memory_actions": memory_actions,

    #     "memory_poses": memory_poses,

    #     "memory_c2w": memory_c2w,

    #     "memory_frame_idx": memory_frame_idx,

    # }



    # np.savez(os.path.join(folder_path, "data_bundle.npz"), **data_dict)



    return last_frame, temporal_video_path, input_history, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx



def reset(selected_image):

    memory_latent_frames = None

    memory_poses = None

    memory_actions = None

    memory_c2w = None

    memory_frame_idx = None

    video_frames = load_image_as_tensor(selected_image).numpy()[None]

    input_history = ""



    new_frame, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx = run_interactive(video_frames[0],

                                actions[0],

                                poses[0],

                                device=device,

                                memory_latent_frames=memory_latent_frames,

                                memory_actions=memory_actions,

                                memory_poses=memory_poses,

                                memory_c2w=memory_c2w,

                                memory_frame_idx=memory_frame_idx,

                                )



    return input_history, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx



def on_image_click(selected_image):

    input_history, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx = reset(selected_image)

    return input_history, selected_image, selected_image, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx



def set_memory(examples_case):

    if examples_case == '1':

        data_bundle = np.load("assets/examples/case1.npz")

        input_history = data_bundle['input_history'].item()

        video_frames = data_bundle['memory_frames']

        memory_latent_frames = data_bundle['self_frames']

        memory_actions = data_bundle['self_actions']

        memory_poses = data_bundle['self_poses']

        memory_c2w = data_bundle['self_memory_c2w']

        memory_frame_idx = data_bundle['self_frame_idx']

    elif examples_case == '2':

        data_bundle = np.load("assets/examples/case2.npz")

        input_history = data_bundle['input_history'].item()

        video_frames = data_bundle['memory_frames']

        memory_latent_frames = data_bundle['self_frames']

        memory_actions = data_bundle['self_actions']

        memory_poses = data_bundle['self_poses']

        memory_c2w = data_bundle['self_memory_c2w']

        memory_frame_idx = data_bundle['self_frame_idx']

    elif examples_case == '3':

        data_bundle = np.load("assets/examples/case3.npz")

        input_history = data_bundle['input_history'].item()

        video_frames = data_bundle['memory_frames']

        memory_latent_frames = data_bundle['self_frames']

        memory_actions = data_bundle['self_actions']

        memory_poses = data_bundle['self_poses']

        memory_c2w = data_bundle['self_memory_c2w']

        memory_frame_idx = data_bundle['self_frame_idx']

    elif examples_case == '4':

        data_bundle = np.load("assets/examples/case4.npz")

        input_history = data_bundle['input_history'].item()

        video_frames = data_bundle['memory_frames']

        memory_latent_frames = data_bundle['self_frames']

        memory_actions = data_bundle['self_actions']

        memory_poses = data_bundle['self_poses']

        memory_c2w = data_bundle['self_memory_c2w']

        memory_frame_idx = data_bundle['self_frame_idx']



    out_video = video_frames.transpose(0,2,3,1)

    out_video = np.clip(out_video, a_min=0.0, a_max=1.0)

    out_video = (out_video * 255).astype(np.uint8)



    temporal_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name

    save_video(out_video, temporal_video_path)



    return input_history, out_video[-1], temporal_video_path, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx



css = """

h1 {

    text-align: center;

    display:block;

}

"""



with gr.Blocks(css=css) as demo:

    gr.Markdown(

        """

        # WORLDMEM: Long-term Consistent World Simulation with Memory

        """

        )



    gr.Markdown(

        """

        ## 🚀 How to Explore WorldMem



        Follow these simple steps to get started:



        1. **Choose a scene**.

        2. **Input your action sequence**.

        3. **Click "Generate"**.



        - You can continuously click **"Generate"** to **extend the video** and observe how well the world maintains consistency over time.

        - For best performance, we recommend **running locally** (1s/frame on H100) instead of Spaces (5s/frame).

        - ⭐️ If you like this project, please [give it a star on GitHub]()!

        - 💬 For questions or feedback, feel free to open an issue or email me at **zeqixiao1@gmail.com**.



        Happy exploring! 🌍

        """

    )

        # <div style="text-align: center;">

        # <!-- Public Website -->

        # <a style="display:inline-block" href="https://nirvanalan.github.io/projects/GA/">

        #     <img src="https://img.shields.io/badge/public_website-8A2BE2">

        # </a>



        # <!-- GitHub Stars -->

        # <a style="display:inline-block; margin-left: .5em" href="https://github.com/NIRVANALAN/GaussianAnything">

        #     <img src="https://img.shields.io/github/stars/NIRVANALAN/GaussianAnything?style=social">

        # </a>



        # <!-- Project Page -->

        # <a style="display:inline-block; margin-left: .5em" href="https://nirvanalan.github.io/projects/GA/">

        #     <img src="https://img.shields.io/badge/project_page-blue">

        # </a>



        # <!-- arXiv Paper -->

        # <a style="display:inline-block; margin-left: .5em" href="https://arxiv.org/abs/XXXX.XXXXX">

        #     <img src="https://img.shields.io/badge/arXiv-paper-red">

        # </a>

        # </div>



    example_actions = {"turn left→turn right": "AAAAAAAAAAAADDDDDDDDDDDD", 

                        "turn 360 degree": "AAAAAAAAAAAAAAAAAAAAAAAA", 

                        "turn right→go backward→look up→turn left→look down": "DDDDDDDDEEEEEEEEEESSSAAAAAAAAWWW", 

                        "turn right→go forward→turn right": "DDDDDDDDDDDDQQQQQQQQQQQQQQQDDDDDDDDDDDD", 

                        "turn right→look up→turn right→look down": "DDDDWWWDDDDDDDDDDDDDDDDDDDDSSS", 

                        "put item→go backward→put item→go backward":"SSUNNWWEEEEEEEEEAAASSUNNWWEEEEEEEEE"}



    selected_image = gr.State(ICE_PLAINS_IMAGE)



    with gr.Row(variant="panel"):

        with gr.Column():

            gr.Markdown("🖼️ Start from this frame.")

            image_display = gr.Image(value=selected_image.value, interactive=False, label="Current Frame")

        with gr.Column():

            gr.Markdown("🎞️ Generated videos. New contents are marked in red box.")

            video_display = gr.Video(autoplay=True, loop=True)



    gr.Markdown("### 🏞️ Choose a scene and start generation.")



    with gr.Row():

        image_display_1 = gr.Image(value=SUNFLOWERS_IMAGE, interactive=False, label="Sunflower Plains")

        image_display_2 = gr.Image(value=DESERT_IMAGE, interactive=False, label="Desert")

        image_display_3 = gr.Image(value=SAVANNA_IMAGE, interactive=False, label="Savanna")

        image_display_4 = gr.Image(value=ICE_PLAINS_IMAGE, interactive=False, label="Ice Plains")

        image_display_5 = gr.Image(value=SUNFLOWERS_RAIN_IMAGE, interactive=False, label="Rainy Sunflower Plains")

        image_display_6 = gr.Image(value=PLACE_IMAGE, interactive=False, label="Place")        





    with gr.Row(variant="panel"):

        with gr.Column(scale=2):

            gr.Markdown("### 🕹️ Input action sequences for interaction.")

            input_box = gr.Textbox(label="Action Sequences", placeholder="Enter action sequences here, e.g. (AAAAAAAAAAAADDDDDDDDDDDD)", lines=1, max_lines=1)

            log_output = gr.Textbox(label="History Sequences", interactive=False)

            gr.Markdown(

                """

                ### 💡 Action Key Guide



                <pre style="font-family: monospace; font-size: 14px; line-height: 1.6;">

                W: Turn up      S: Turn down     A: Turn left     D: Turn right

                Q: Go forward   E: Go backward   N: No-op         U: Use item

                </pre>

                """

            )

            gr.Markdown("### 👇 Click to quickly set action sequence examples.")

            with gr.Row():

                buttons = []

                for action_key in list(example_actions.keys())[:2]:

                    with gr.Column(scale=len(action_key)):

                        buttons.append(gr.Button(action_key))

            with gr.Row():

                for action_key in list(example_actions.keys())[2:4]:

                    with gr.Column(scale=len(action_key)):

                        buttons.append(gr.Button(action_key))

            with gr.Row():

                for action_key in list(example_actions.keys())[4:6]:

                    with gr.Column(scale=len(action_key)):

                        buttons.append(gr.Button(action_key))



        with gr.Column(scale=1):

            submit_button = gr.Button("🎬 Generate!", variant="primary")

            reset_btn = gr.Button("🔄 Reset")



            # gr.Markdown("<div style='flex-grow:1; height: 100px'></div>")



            gr.Markdown("### ⚙️ Advanced Settings")



            slider_denoising_step = gr.Slider(

                minimum=10, maximum=50, value=worldmem.sampling_timesteps, step=1,

                label="Denoising Steps",

                info="Higher values yield better quality but slower speed"

            )

            slider_context_length = gr.Slider(

                minimum=2, maximum=10, value=worldmem.n_tokens, step=1,

                label="Context Length",

                info="How many previous frames in temporal context window."

            )

            slider_memory_condition_length = gr.Slider(

                minimum=4, maximum=16, value=worldmem.memory_condition_length, step=1,

                label="Memory Length",

                info="How many previous frames in memory window. (Recommended: 1, multi-frame generation is not stable yet)"

            )

            slider_next_frame_length = gr.Slider(

                minimum=1, maximum=5, value=worldmem.next_frame_length, step=1,

                label="Next Frame Length",

                info="How many next frames to generate at once."

            )

    

    sampling_timesteps_state = gr.State(worldmem.sampling_timesteps)

    sampling_context_length_state = gr.State(worldmem.n_tokens)

    sampling_memory_condition_length_state = gr.State(worldmem.memory_condition_length)

    sampling_next_frame_length_state = gr.State(worldmem.next_frame_length)



    video_frames = gr.State(load_image_as_tensor(selected_image.value)[None].numpy())

    memory_latent_frames = gr.State()

    memory_actions = gr.State()

    memory_poses = gr.State()

    memory_c2w = gr.State()

    memory_frame_idx = gr.State()



    def set_action(action):

        return action

    





    for button, action_key in zip(buttons, list(example_actions.keys())):

            button.click(set_action, inputs=[gr.State(value=example_actions[action_key])], outputs=input_box)



    gr.Markdown("### 👇 Click to review generated examples, and continue generation based on them.")



    example_case = gr.Textbox(label="Case", visible=False)

    image_output = gr.Image(visible=False) 



    examples = gr.Examples(

        examples=example_images,

        inputs=[example_case, image_output, log_output, slider_denoising_step, slider_context_length, slider_memory_condition_length],

        cache_examples=False

    )



    example_case.change(

        fn=set_memory,

        inputs=[example_case],

        outputs=[log_output, image_display, video_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx]

    )



    submit_button.click(generate, inputs=[input_box, log_output, video_frames, 

                                          memory_latent_frames, memory_actions, memory_poses, 

                                          memory_c2w, memory_frame_idx], 

                                          outputs=[image_display, video_display, log_output, 

                                                    video_frames, memory_latent_frames, memory_actions, memory_poses, 

                                                    memory_c2w, memory_frame_idx])



    reset_btn.click(reset, inputs=[selected_image], outputs=[log_output, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_1.select(lambda: on_image_click(SUNFLOWERS_IMAGE), outputs=[log_output, selected_image, image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_2.select(lambda: on_image_click(DESERT_IMAGE), outputs=[log_output, selected_image, image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_3.select(lambda: on_image_click(SAVANNA_IMAGE), outputs=[log_output, selected_image, image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_4.select(lambda: on_image_click(ICE_PLAINS_IMAGE), outputs=[log_output, selected_image, image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_5.select(lambda: on_image_click(SUNFLOWERS_RAIN_IMAGE), outputs=[log_output, selected_image, image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])

    image_display_6.select(lambda: on_image_click(PLACE_IMAGE), outputs=[log_output, selected_image,image_display, video_frames, memory_latent_frames, memory_actions, memory_poses, memory_c2w, memory_frame_idx])



    slider_denoising_step.change(fn=set_denoising_steps, inputs=[slider_denoising_step, sampling_timesteps_state], outputs=sampling_timesteps_state)

    slider_context_length.change(fn=set_context_length, inputs=[slider_context_length, sampling_context_length_state], outputs=sampling_context_length_state)

    slider_memory_condition_length.change(fn=set_memory_condition_length, inputs=[slider_memory_condition_length, sampling_memory_condition_length_state], outputs=sampling_memory_condition_length_state)

    slider_next_frame_length.change(fn=set_next_frame_length, inputs=[slider_next_frame_length, sampling_next_frame_length_state], outputs=sampling_next_frame_length_state)



demo.launch(share=True)



